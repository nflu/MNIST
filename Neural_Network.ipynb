{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ..., 8 4 8]\n"
     ]
    }
   ],
   "source": [
    "import _pickle as cPickle\n",
    "import gzip \n",
    "import numpy as np\n",
    "import sys \n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f,encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "print(train_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-449-817d7421a1fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "    return np.exp(x)/((1+np.exp(x))**2)\n",
    "\n",
    "def relu(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def reluPrime(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def elementWise(f, x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = f(x[i])\n",
    "    return x    \n",
    "\n",
    "def softMax(x):\n",
    "    z = np.exp(x)\n",
    "    return z/sum(z)\n",
    "\n",
    "def softMaxPrime(x):\n",
    "    z = np.exp(x)\n",
    "    c = sum(z)\n",
    "    for i in range(len(z)):\n",
    "        z[i] = (c-z[i])*z[i]/(c**2)\n",
    "    return z\n",
    "\n",
    "def display(x, label, act):\n",
    "    strn = \"\"\n",
    "    for i in range(len(x)):\n",
    "        if i%28==0:\n",
    "            print(strn)\n",
    "            strn = \"\"\n",
    "        if x[i]==0:\n",
    "            strn += \" \"\n",
    "        if x[i]>=act:\n",
    "            strn += \"x\"\n",
    "    print(label)\n",
    "            \n",
    "def logLoss(x, target):\n",
    "    loss = 0\n",
    "    for i in range(len(x)):\n",
    "        loss += target[i]*np.log(x[i])+(1-target[i])*np.log(1-x[i])\n",
    "    return (-1.0/len(x))*loss\n",
    "    \n",
    "def logLossPrime(x, target):\n",
    "    grad = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        grad[i]=(-1.0/len(x))*(target[i]/x[i]-(1-target[i])/(1-x[i]))\n",
    "    return grad\n",
    "    \n",
    "class Neural_Network:\n",
    "    defaultSize = 16\n",
    "    inputChecks = True #this will change whether inputs that match the MNIST format are given\n",
    "    #can be turned off to allow for debugging on smaller examples\n",
    "    \n",
    "    def __init__(self, activation, activationPrime, shape = None, layers = 2):\n",
    "        if layers<2:\n",
    "            raise NameError(\"two few layers\")\n",
    "        if Neural_Network.inputChecks and (shape[0] != 28**2 or shape[len(shape)-1] != 10):\n",
    "            raise NameError(\"improper input or output layer \")\n",
    "        if shape==None:\n",
    "            shape = [28**2] + [defaultSize for i in range(layers-2)] + [10]\n",
    "        self.activation = activation\n",
    "        self.activationPrime = activationPrime\n",
    "        self.shape = shape\n",
    "        self.weights = Neural_Network.constructWeights(shape) \n",
    "        #weights[i] is the weights going into layer i\n",
    "        self.bias = Neural_Network.constructBias(shape)\n",
    "        #bias[i] is the bias on layer i \n",
    "        self.activations = []\n",
    "        self.zs = []\n",
    "        \n",
    "    #returns a list of matrices of weights. weights[i] is the set of weights going into ith layer\n",
    "    #weights[i][j][k] represents the weight going from the kth neuron in layer i-1 to jth neuron in layer i \n",
    "    def constructWeights(shape):\n",
    "        weights = [None]\n",
    "        for i in range(len(shape)-1): \n",
    "            weights.append(np.random.uniform(-1,1,shape[i]*shape[i+1]).reshape((shape[i+1],shape[i])))\n",
    "        return weights\n",
    "    \n",
    "    #returns a list of vectors of biases. bias[i] is the set of biases on the ith layer\n",
    "    #bias[i][j] is the bias in the ith layer on the jth neuron\n",
    "    def constructBias(shape):\n",
    "        bias = [None]\n",
    "        for i in range(1,len(shape)): \n",
    "            bias.append(np.random.uniform(-1,1,shape[i]))\n",
    "        return bias\n",
    "    \n",
    "    #performs forward propagation on the input X with the current weights and biases\n",
    "    #using activation function from the constructor returns the activations on the last layer \n",
    "    #updates the zs and activations attributes\n",
    "    def forwardProp(self, X):\n",
    "        if Neural_Network.inputChecks and len(X) != 28**2:\n",
    "            raise NameError(\"improper input\")\n",
    "        prevAct = X\n",
    "        act = []\n",
    "        self.activations = []\n",
    "        self.zs = []\n",
    "        self.activations.append(prevAct)\n",
    "        for i in range(1,len(self.shape)): #each layer\n",
    "            z = np.dot(self.weights[i], prevAct) + self.bias[i]\n",
    "            act = elementWise(self.activation, z)\n",
    "            self.activations.append(act)\n",
    "            self.zs.append(z)\n",
    "            prevAct = act\n",
    "        return act\n",
    "    \n",
    "    \n",
    "    def costFunctionCrossEntropy(self, data):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        if len(images) != len(labels):\n",
    "            raise NameError(\"improper input\")\n",
    "        target = np.zeros(10)\n",
    "        cost = 0 \n",
    "        for i in range(len(images)):\n",
    "            target[labels[i]] = 1\n",
    "            cost += Neural_Network.crossEntropy(softMax(self.forwardProp(images[i])),target)\n",
    "            target[labels[i]] = 0\n",
    "        return cost\n",
    "    \n",
    "    def crossEntropy(output, target):\n",
    "        cost = 0\n",
    "        for i in range(len(target)):\n",
    "            cost -= target[i]*np.log(output[i])\n",
    "        return cost\n",
    "    \n",
    "    def costFunctionEuclid(self, data):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        if Neural_Network.inputChecks and len(images) != len(labels):\n",
    "            raise NameError(\"improper input\")\n",
    "        target = np.zeros(10)\n",
    "        cost = 0 \n",
    "        for i in range(len(images)):\n",
    "            target[labels[i]] = 1\n",
    "            out = softMax(self.forwardProp(images[i]))\n",
    "            cost += np.sum(out-target)**2\n",
    "            target[labels[i]] = 0\n",
    "        return cost\n",
    "    \n",
    "    def costFunctionLogLoss(self, data):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        if Neural_Network.inputChecks and len(images) != len(labels):\n",
    "            raise NameError(\"improper input\")\n",
    "        target = np.zeros(10)\n",
    "        cost = 0 \n",
    "        for i in range(len(images)):\n",
    "            target[labels[i]] = 1\n",
    "            out = self.forwardProp(images[i])\n",
    "            cost += logLoss(out, target)\n",
    "            target[labels[i]] = 0\n",
    "        return cost\n",
    "        \n",
    "    def classification(outputActivations):\n",
    "        maximum = -1\n",
    "        index = -1\n",
    "        for i in range(len(outputActivations)):\n",
    "            if outputActivations[i] >= maximum:\n",
    "                index = i\n",
    "                maximum = outputActivations[i]\n",
    "        return index\n",
    "                \n",
    "    def randomInitialization(self):\n",
    "        self.weights = Neural_Network.constructWeights(self.shape) \n",
    "        self.bias = Neural_Network.constructBias(self.shape)\n",
    "    \n",
    "    def improvement(prev, new, target):\n",
    "        return sum(target - new)**2 <= sum(target - prev)**2\n",
    "    \n",
    "    def test(self, data):\n",
    "        print(\"test\")\n",
    "        correct = 0\n",
    "        wrong = 0\n",
    "        target = np.zeros(10)\n",
    "        for i in range(20):\n",
    "            prevAct = self.forwardProp(data[0][i])\n",
    "            target[data[1][i]] = 1\n",
    "            grad = self.backProp(target)\n",
    "            target[data[1][i]] = 0\n",
    "            self.applyGradient(grad, .1)\n",
    "            if Neural_Network.improvement(prevAct, self.forwardProp(data[0][i]), target):\n",
    "                correct +=1\n",
    "            else:\n",
    "                wrong +=1\n",
    "            self.randomInitialization()\n",
    "        print(\"correct: \", correct)\n",
    "        print(\"wrong: \", wrong)\n",
    "    \n",
    "    #using backProp this will perform gradient descent from the current initialization of the weights\n",
    "    #and biases on the given data. data in form touple of array of images and array of labels\n",
    "    #will stop when the change in cost between steps is less than epsilon\n",
    "    def gradientDescent(self, data, epsilon, learningRate):\n",
    "        return self.stochasticGradientDescent(data, len(data[0]), epsilon, learningRate)\n",
    "    \n",
    "    #will apply the gradient with stepSize where the gradient is in form \n",
    "    #given by the backProp function\n",
    "    def applyGradient(self, gradient, stepSize):\n",
    "        for i in range(1, len(gradient[0])): #for each matrix in the weight update, \n",
    "            #first value is None for convenience in indexing\n",
    "            self.weights[i] -= stepSize * gradient[0][i]\n",
    "        for i in range(1, len(gradient[1])): #for each vector in the bias update\n",
    "            self.bias[i] -= stepSize * gradient[1][i]\n",
    "        \n",
    "    def validation(self, data):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        if len(images) != len(labels):\n",
    "            raise NameError(\"improper input\")\n",
    "        correct = 0 \n",
    "        wrong = 0\n",
    "        for i in range(len(images)):\n",
    "            if (Neural_Network.classification(self.forwardProp(images[i]))) == labels[i]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        print(\"correct: \", correct)\n",
    "        print(\"wrong: \", wrong)\n",
    "    \n",
    "    def randomBatch(images, labels, batchSize):\n",
    "        indices = np.random.choice(len(images), batchSize, replace = False)\n",
    "        imageBatch = []\n",
    "        labelBatch = []\n",
    "        for i in range(len(indices)):\n",
    "            imageBatch.append(images[indices[i]])\n",
    "            labelBatch.append(labels[indices[i]])\n",
    "        return (imageBatch, labelBatch)\n",
    "        \n",
    "    #using backProp this will perform gradient descent from the current initialization of the weights\n",
    "    #and biases on the given data, with the given batch size \n",
    "    def stochasticGradientDescent(self, data, batchSize, epsilon, learningRate):\n",
    "        prevCost = sys.maxsize\n",
    "        cost = 0\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        while prevCost - cost >= epsilon:\n",
    "            for i in range(5):\n",
    "                print(i)\n",
    "                imageSet, labelSet = Neural_Network.randomBatch(images, labels, batchSize)\n",
    "                target = np.zeros(10)\n",
    "                gradients = []\n",
    "                for i in range(len(imageSet)):\n",
    "                    target[labelSet[i]] = 1\n",
    "                    output = self.forwardProp(imageSet[i])\n",
    "                    gradients.append(self.backProp(target))\n",
    "                    target[labelSet[i]] = 0\n",
    "                self.applyGradient(self.averageGradient(gradients), learningRate)\n",
    "            cost = self.costFunctionLogLoss(data)\n",
    "            self.validation(data)\n",
    "            print(cost)\n",
    "            prevCost = cost# oh hmm this needs a change\n",
    "            cost = 0\n",
    "    \n",
    "    #given a list of touples of the gradient in the form given by backProp\n",
    "    def averageGradient(self, gradients):\n",
    "        averageGrad = (Neural_Network.constructWeights(self.shape), Neural_Network.constructBias(self.shape))\n",
    "        for i in range(len(averageGrad)): #weights then biases\n",
    "            for j in range(len(gradients)): #grad from each sample\n",
    "                for k in range(1, len(gradients[j][i])): #grad for each matrix\n",
    "                    averageGrad[i][k] += gradients[j][i][k]\n",
    "            for k in range(1,len(averageGrad[i])):#done at end to prevent rounding small numbers to zero\n",
    "                averageGrad[i][k] /= len(gradients) #if overflow is a problem then divide at each step\n",
    "        return averageGrad\n",
    "    \n",
    "    #this will return the gradient of the cost on the single target\n",
    "    #the form is a touple with the weights and then the biases\n",
    "    #in the same format as the weights and biases attributes for the Neural_Network class\n",
    "    def backProp(self, target):\n",
    "        if Neural_Network.inputChecks and len(target) != 10:\n",
    "            raise NameError(\"improper input\")\n",
    "        #computes the gradient of the cost with respect to each of the activations \n",
    "        #caches results for use later\n",
    "        def activationGrad():\n",
    "            actGrad = []\n",
    "            output = self.activations[len(self.shape)-1]\n",
    "            layerActGrad = logLossPrime(output, target)#last layer using log loss\n",
    "            actGrad.append(layerActGrad)\n",
    "            for i in range(len(self.shape)-2, 0, -1):#each layer backwards from second to last one\n",
    "                prevLayerActGrad = layerActGrad\n",
    "                layerActGrad = np.zeros(self.shape[i])\n",
    "                for k in range(self.shape[i]):#each neuron in layer i\n",
    "                    for j in range(self.shape[i+1]): #each nueron in layer i+1\n",
    "                        layerActGrad[k] += self.weights[i+1][j][k]*self.activationPrime(self.zs[i][j])*prevLayerActGrad[j]\n",
    "                          #TODO figure out which weight\n",
    "                actGrad.append(layerActGrad)\n",
    "            actGrad.append(None) #needed to avoid off by one error\n",
    "            actGrad.reverse()\n",
    "            return actGrad\n",
    "        \n",
    "        actGrad = activationGrad()\n",
    "        weightGrad = Neural_Network.constructWeights(self.shape) #kept in list of matrix form\n",
    "        biasGrad = Neural_Network.constructBias(self.shape) #kept in list of vector form\n",
    "        for i in range(len(self.shape)-1, 1, -1): #loop from last layer to first\n",
    "            for j in range(self.shape[i]): #each neuron\n",
    "                for k in range(len(self.weights[i-1])):#each weight #TODO this needs to be adjusted now that in matrix form\n",
    "                    deriv = self.activationPrime(self.zs[i-1][j])\n",
    "                    actDeriv = actGrad[i][j]\n",
    "                    a = self.activations[i][j] * deriv * actDeriv#TODO check i-1\n",
    "                    weightGrad[i][j][k] = a\n",
    "                    #TODO here I need to use the activationsGrad\n",
    "                biasGrad[i][j] = self.activationPrime(self.zs[i-1][j])*actGrad[i][j] #each bias on each neuron\n",
    "        return (weightGrad, biasGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "metadata": {},
   "outputs": [],
   "source": [
    "Neural_Network.inputChecks = True\n",
    "a = Neural_Network(sigmoid, sigmoidPrime, shape=[28**2, 8, 8, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.71090105,  0.48020259,  0.36616042,  0.51390308,  0.44072154,\n",
       "        0.3560126 ,  0.511882  ,  0.49526543,  0.5496971 ,  0.67491579])"
      ]
     },
     "execution_count": 1264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.forwardProp(train_set[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1266-74d743c5ebfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochasticGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1249-ffe7f5722a32>\u001b[0m in \u001b[0;36mstochasticGradientDescent\u001b[0;34m(self, data, batchSize, epsilon, learningRate)\u001b[0m\n\u001b[1;32m    252\u001b[0m                     \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabelSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                     \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m                     \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabelSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverageGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1249-ffe7f5722a32>\u001b[0m in \u001b[0;36mbackProp\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mactGrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mactGrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivationGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0mweightGrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeural_Network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstructWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#kept in list of matrix form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mbiasGrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeural_Network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstructBias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#kept in list of vector form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1249-ffe7f5722a32>\u001b[0m in \u001b[0;36mactivationGrad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#each neuron in layer i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#each nueron in layer i+1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                         \u001b[0mlayerActGrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivationPrime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mprevLayerActGrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m                           \u001b[0;31m#TODO figure out which weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mactGrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayerActGrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1249-ffe7f5722a32>\u001b[0m in \u001b[0;36msigmoidPrime\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoidPrime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a.stochasticGradientDescent(train_set, 2000, 0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[ 0.19473711  0.10362538  0.18675223  0.177155    0.13337837  0.1735821\n",
      "  0.1677258   0.1956107   0.13970612  0.10809757]\n",
      "[ 0.12322245  0.06557031  0.11816991  0.11209714  0.0843969   0.10983634\n",
      "  0.10613069  0.12377522  0.08840088  0.06840015]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(train_set[1][39])\n",
    "print(a.forwardProp(train_set[0][39]))\n",
    "print(a.forwardProp(train_set[0][39])/sum(a.forwardProp(train_set[0][39])))\n",
    "print(Neural_Network.classification(a.forwardProp(train_set[0][39])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max 0.209606395907 mean 0.165720237187\n",
      "\n",
      "max 0.211897986483 mean 0.165926005147\n",
      "\n",
      "max 0.200760577901 mean 0.148790578021\n",
      "\n",
      "max 0.222265083679 mean 0.178058103785\n",
      "\n",
      "max 0.202868356238 mean 0.164999853832\n",
      "\n",
      "max 0.219064038745 mean 0.177485227495\n",
      "\n",
      "max 0.228872610102 mean 0.170549522648\n",
      "\n",
      "max 0.201728441173 mean 0.162691786696\n",
      "\n",
      "max 0.209206726755 mean 0.15799747965\n",
      "\n",
      "max 0.187287013279 mean 0.145770764122\n",
      "\n",
      "max 0.233170572858 mean 0.189571256659\n",
      "\n",
      "max 0.260396896273 mean 0.181724946789\n",
      "\n",
      "max 0.234279376586 mean 0.166940012199\n",
      "\n",
      "max 0.215835768643 mean 0.165287165502\n",
      "\n",
      "max 0.228810126651 mean 0.162833296449\n",
      "\n",
      "max 0.208536108784 mean 0.148583232869\n",
      "\n",
      "max 0.221358946557 mean 0.17057841095\n",
      "\n",
      "max 0.234290075471 mean 0.186349396344\n",
      "\n",
      "max 0.174236041092 mean 0.141690758922\n",
      "\n",
      "max 0.199570217522 mean 0.166938359855\n",
      "\n",
      "max 0.223299153026 mean 0.165671981377\n",
      "\n",
      "max 0.216158032076 mean 0.168713028994\n",
      "\n",
      "max 0.193139491243 mean 0.153845007375\n",
      "\n",
      "max 0.215498963467 mean 0.177476232856\n",
      "\n",
      "max 0.23030137239 mean 0.177145362664\n",
      "\n",
      "max 0.21759086056 mean 0.169376556901\n",
      "\n",
      "max 0.212797361126 mean 0.15384554411\n",
      "\n",
      "max 0.195782489002 mean 0.151521575402\n",
      "\n",
      "max 0.185092681471 mean 0.143986045587\n",
      "\n",
      "max 0.205521333947 mean 0.163844919369\n",
      "\n",
      "max 0.236372054004 mean 0.167045296551\n",
      "\n",
      "max 0.222290460919 mean 0.162667509643\n",
      "\n",
      "max 0.207798318048 mean 0.159976418365\n",
      "\n",
      "max 0.200679196234 mean 0.154311219463\n",
      "\n",
      "max 0.193508528938 mean 0.148009695279\n",
      "\n",
      "max 0.269457676586 mean 0.189563680497\n",
      "\n",
      "max 0.214761790246 mean 0.160692547594\n",
      "\n",
      "max 0.207113747378 mean 0.163329329839\n",
      "\n",
      "max 0.236727119617 mean 0.174531100398\n",
      "\n",
      "max 0.195610697442 mean 0.158037037416\n",
      "\n",
      "max 0.247654525475 mean 0.187253355239\n",
      "\n",
      "max 0.204802870978 mean 0.172622223496\n",
      "\n",
      "max 0.230384072768 mean 0.164435938775\n",
      "\n",
      "max 0.242767362364 mean 0.187123255767\n",
      "\n",
      "max 0.230549229309 mean 0.184395467136\n",
      "\n",
      "max 0.204283156179 mean 0.155412954195\n",
      "\n",
      "max 0.25057677208 mean 0.190302677508\n",
      "\n",
      "max 0.194977641855 mean 0.156318426439\n",
      "\n",
      "max 0.229146895914 mean 0.179448785983\n",
      "\n",
      "max 0.209056401107 mean 0.162954857636\n",
      "\n",
      "max 0.195525583272 mean 0.161893464192\n",
      "\n",
      "max 0.229944710874 mean 0.149305434013\n",
      "\n",
      "max 0.187072185451 mean 0.142045692505\n",
      "\n",
      "max 0.264959669481 mean 0.19253075979\n",
      "\n",
      "max 0.182576207799 mean 0.141623231472\n",
      "\n",
      "max 0.200784875696 mean 0.160320992698\n",
      "\n",
      "max 0.272415549537 mean 0.181618464698\n",
      "\n",
      "max 0.197720530164 mean 0.156655469155\n",
      "\n",
      "max 0.208664671792 mean 0.162082111456\n",
      "\n",
      "max 0.232236460376 mean 0.177016250831\n",
      "\n",
      "max 0.20005800787 mean 0.147965335089\n",
      "\n",
      "max 0.201783709928 mean 0.167519232527\n",
      "\n",
      "max 0.200046795404 mean 0.1429876115\n",
      "\n",
      "max 0.207110430406 mean 0.163305112962\n",
      "\n",
      "max 0.221296644408 mean 0.166397043369\n",
      "\n",
      "max 0.252629840214 mean 0.178979764992\n",
      "\n",
      "max 0.207113926704 mean 0.16344993666\n",
      "\n",
      "max 0.250892354139 mean 0.175610609995\n",
      "\n",
      "max 0.258878082021 mean 0.181520787325\n",
      "\n",
      "max 0.217835622738 mean 0.182755337141\n",
      "\n",
      "max 0.240565805683 mean 0.173378648824\n",
      "\n",
      "max 0.20579169583 mean 0.148728220208\n",
      "\n",
      "max 0.221228111813 mean 0.168453137303\n",
      "\n",
      "max 0.21015137861 mean 0.16459964475\n",
      "\n",
      "max 0.27485086462 mean 0.191284409972\n",
      "\n",
      "max 0.229909175509 mean 0.187688840432\n",
      "\n",
      "max 0.228949818871 mean 0.18341327309\n",
      "\n",
      "max 0.251949806309 mean 0.188334097816\n",
      "\n",
      "max 0.259878494108 mean 0.189889854739\n",
      "\n",
      "max 0.279503021682 mean 0.179333153214\n",
      "\n",
      "max 0.199702949037 mean 0.158147959297\n",
      "\n",
      "max 0.209115687282 mean 0.16543186505\n",
      "\n",
      "max 0.195373677753 mean 0.156554341174\n",
      "\n",
      "max 0.20409769213 mean 0.162034234409\n",
      "\n",
      "max 0.195790074641 mean 0.140385276943\n",
      "\n",
      "max 0.211466942035 mean 0.163729723354\n",
      "\n",
      "max 0.231159196594 mean 0.164589113579\n",
      "\n",
      "max 0.19028864926 mean 0.145640561356\n",
      "\n",
      "max 0.245942753728 mean 0.168914857977\n",
      "\n",
      "max 0.180747423757 mean 0.145362934893\n",
      "\n",
      "max 0.210985871733 mean 0.162949305404\n",
      "\n",
      "max 0.187663484822 mean 0.143783996018\n",
      "\n",
      "max 0.20380299027 mean 0.162923871073\n",
      "\n",
      "max 0.206692774371 mean 0.162908863737\n",
      "\n",
      "max 0.203916537122 mean 0.161027506856\n",
      "\n",
      "max 0.226324575586 mean 0.182975439307\n",
      "\n",
      "max 0.251368570201 mean 0.176027933909\n",
      "\n",
      "max 0.208067018959 mean 0.161830583108\n",
      "\n",
      "max 0.246992817462 mean 0.190013843539\n",
      "\n",
      "max 0.247780226084 mean 0.187287676599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    out = a.forwardProp(train_set[0][i])\n",
    "    print(\"max\", max(out), \"mean\", np.mean(out))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1211-5edb7b4ffa78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "numpy.random.choice(5, 6, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  1341\n",
      "wrong:  8659\n"
     ]
    }
   ],
   "source": [
    "a.validation(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  1394\n",
      "wrong:  8606\n"
     ]
    }
   ],
   "source": [
    "a.validation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
