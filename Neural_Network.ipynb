{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and data input\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as cPickle\n",
    "import pickle\n",
    "import gzip \n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "from scipy.special import expit\n",
    "\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f,encoding='latin1')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are just various mathematical tools needed for the NN\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def identityPrime(x):\n",
    "    return np.ones(len(x))\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "    return expit(x)*(1-expit(x)) #use expit to prevent overflow with large values\n",
    "\n",
    "def sigmoid(x):\n",
    "    return expit(x) #admittedly this is a little unnecessary but I think it makes sense to have \n",
    "    #sigmoid and sigmoidPrime instead of expit and sigmoidPrime\n",
    "\n",
    "def relu(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def reluPrime(x):\n",
    "    z = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        if x[i]>0:\n",
    "            z[i] = 1\n",
    "    return z\n",
    "\n",
    "def elementWise(f, x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = f(x[i])\n",
    "    return x    \n",
    "\n",
    "def softMax(x):\n",
    "    z = np.exp(x)\n",
    "    return z/sum(z)\n",
    "\n",
    "def softMaxPrime(x):\n",
    "    z = np.exp(x)\n",
    "    c = sum(z)\n",
    "    for i in range(len(z)):\n",
    "        z[i] = (c-z[i])*z[i]/(c*c)\n",
    "    return z\n",
    "\n",
    "def logLoss(x, target):\n",
    "    loss = 0\n",
    "    for i in range(len(x)):\n",
    "        loss += target[i]*np.log(x[i])+(1-target[i])*np.log(1-x[i])\n",
    "    return (-1.0/len(x))*loss\n",
    "    \n",
    "def logLossPrime(x, target):\n",
    "    grad = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        grad[i]=(-1.0/len(x))*(target[i]/x[i]-(1-target[i])/(1-x[i]))\n",
    "    return grad\n",
    "\n",
    "def MSE(x, target):\n",
    "    a = x-target\n",
    "    return np.dot(a,a)\n",
    "\n",
    "def MSEPrime(x, target):\n",
    "    return 2*(x-target)\n",
    "\n",
    "def crossEntropy(output, target):\n",
    "    cost = 0\n",
    "    for i in range(len(target)):\n",
    "        cost -= target[i]*np.log(output[i])\n",
    "    return cost\n",
    "\n",
    "def crossEntropyPrime(output, target):\n",
    "    return -target/output\n",
    "\n",
    "def softMaxCross(x, target):\n",
    "    return crossEntropy(softMax(x), target)\n",
    "\n",
    "def softMaxCrossPrime(x, target):\n",
    "    return softMax(x)-target\n",
    "\n",
    "def trainingGraph(trainingData):\n",
    "    costs = trainingData[0]\n",
    "    accuracies = np.array(trainingData[1])\n",
    "    plt.plot(costs, 'ro')\n",
    "    plt.ylabel(\"total cost\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.show()\n",
    "    plt.plot(100.0*(1.0-accuracies), 'bo')\n",
    "    plt.ylabel(\"total error percentage\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.show()    \n",
    "\n",
    "class Neural_Network:\n",
    "    defaultSize = 16 #default number of neurons in hidden layers if no shape list given\n",
    "    inputChecks = True #this will change whether inputs that match the MNIST format are given\n",
    "    #can be turned off to allow for debugging on smaller examples\n",
    "    \n",
    "    #activation must be an activation function that works for a single scalar \n",
    "    #and activation prime is its derivative. costFunction is a cost function for a vector representing an output layer\n",
    "    #and the target vector. costDeriv is its derivative with respect to the vector of activations.\n",
    "    #shape is list of integers where shape[i] = the number of neurons in ith layer\n",
    "    #layers is an integer describing the number of layers, lastAct is an optional change so that on the last layer\n",
    "    #you can have a different activation function\n",
    "    #shape and layers are optional. if both are given the shape list will be followed, if neither are given\n",
    "    #it will use the default value for layers and make all hidden layers have defaultSize many neurons\n",
    "    #lastAct and lastActPrime are the activation function and its derivative for the last layer. if left blank\n",
    "    #will default to the given activation and activationPrime\n",
    "    def __init__(self, activation, activationPrime, costFunction, costDeriv, shape = None, layers = 4, \n",
    "                 lastAct = None, lastActPrime = None):\n",
    "        if layers<2:\n",
    "            raise NameError(\"Too few layers. Need an input layer and an output layer.\")\n",
    "        if Neural_Network.inputChecks and (shape[0] != 28**2 or shape[len(shape)-1] != 10):\n",
    "            raise NameError(\"Improper input or output layer size. \\\n",
    "                            Must be 28^2 input neurons and 10 output neurons to work with MNIST.\")\n",
    "        if shape==None:\n",
    "            shape = [28**2] + [defaultSize for i in range(layers-2)] + [10]\n",
    "        self.activation = activation\n",
    "        self.activationPrime = activationPrime\n",
    "        self.shape = shape\n",
    "        self.costFunction = costFunction\n",
    "        self.costDeriv = costDeriv\n",
    "        self.weights = Neural_Network.constructWeights(shape) \n",
    "        #weights[i] is the weights going into layer i\n",
    "        self.bias = Neural_Network.constructBias(shape)\n",
    "        #bias[i] is the bias on layer i\n",
    "        if lastAct == None:\n",
    "            self.lastAct = activation\n",
    "            self.lastActPrime = activationPrime\n",
    "        else:\n",
    "            self.lastAct = lastAct\n",
    "            self.lastActPrime = lastActPrime\n",
    "        self.activations = []\n",
    "        self.zs = []\n",
    "        \n",
    "    #returns a list of matrices of weights. weights[i] is the set of weights going into ith layer\n",
    "    #weights[i][j][k] represents the weight going from the kth neuron in layer i-1 to jth neuron in layer i \n",
    "    def constructWeights(shape):\n",
    "        weights = [None]\n",
    "        for i in range(len(shape)-1): \n",
    "            weights.append(np.random.uniform(-0.1,0.1,shape[i]*shape[i+1]).reshape((shape[i+1],shape[i])))\n",
    "        return weights\n",
    "    \n",
    "    #returns a list of vectors of biases. bias[i] is the set of biases on the ith layer\n",
    "    #bias[i][j] is the bias in the ith layer on the jth neuron\n",
    "    def constructBias(shape):\n",
    "        bias = [None]\n",
    "        for i in range(1,len(shape)): \n",
    "            bias.append(np.random.uniform(-0.1,0.1,shape[i]))\n",
    "        return bias\n",
    "    \n",
    "    #performs forward propagation on the input x with the current weights and biases\n",
    "    #using activation function from the constructor returns the activations on the last layer \n",
    "    #updates the zs and activations attributes\n",
    "    def forwardProp(self, x):\n",
    "        if Neural_Network.inputChecks and len(x) != 28**2:\n",
    "            raise NameError(\"improper input\")\n",
    "        prevAct = x\n",
    "        act = []\n",
    "        self.activations = []\n",
    "        self.zs = []\n",
    "        self.activations.append(prevAct)\n",
    "        for i in range(1,len(self.shape)-1): #each layer except for last\n",
    "            z = np.dot(self.weights[i], prevAct) + self.bias[i]\n",
    "            act = elementWise(self.activation, z)\n",
    "            self.activations.append(act)\n",
    "            self.zs.append(z)\n",
    "            prevAct = act\n",
    "        i = len(self.shape)-1    \n",
    "        z = np.dot(self.weights[i], prevAct) + self.bias[i]\n",
    "        act = elementWise(self.lastAct, z)\n",
    "        self.activations.append(act)\n",
    "        self.zs.append(z)\n",
    "        prevAct = act\n",
    "        return act\n",
    "    \n",
    "    #returns the total cost for the neural network on all datapoints in data\n",
    "    #using cost function costFunc for each point. returns cost as a scalar\n",
    "    def totalCost(self, data, costFunc):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        if Neural_Network.inputChecks and len(images) != len(labels):\n",
    "            raise NameError(\"improper input\")\n",
    "        target = np.zeros(10)\n",
    "        cost = 0 \n",
    "        for i in range(len(images)):\n",
    "            target[labels[i]] = 1\n",
    "            out = self.forwardProp(images[i])\n",
    "            cost += costFunc(out, target)\n",
    "            target[labels[i]] = 0\n",
    "        return cost\n",
    "        \n",
    "    #returns the classification as a scalar based on the outputActivations\n",
    "    #picks the index with highest activation\n",
    "    def classification(outputActivations):\n",
    "        maximum = -1\n",
    "        index = -1\n",
    "        for i in range(len(outputActivations)):\n",
    "            if outputActivations[i] >= maximum:\n",
    "                index = i\n",
    "                maximum = outputActivations[i]\n",
    "        return index\n",
    "     \n",
    "    #randomly initializes all weights and biases    \n",
    "    def randomInitialization(self):\n",
    "        self.weights = Neural_Network.constructWeights(self.shape) \n",
    "        self.bias = Neural_Network.constructBias(self.shape)\n",
    "    \n",
    "    #using backProp this will perform gradient descent from the current initialization of the weights\n",
    "    #and biases on the given data. data in form touple of array of images and array of labels\n",
    "    #will stop after iterations many iterations. returns a touple of the best weights and biases\n",
    "    #and a list of the costs over time. will update the weights and biases in the neural network\n",
    "    #suppressPrint will not print anything between steps if set to true\n",
    "    #costInterval determines how often the cost will be re-computed, saved, printed (if suppressPrint is false)\n",
    "    #and how often the best weights and biases are saved. when set to 1 it occurs on every step\n",
    "    def gradientDescent(self, data, iterations, learningRate, suppressPrint = False, costInterval = 1):\n",
    "        return self.stochasticGradientDescent(data, len(data[0]), iterations, learningRate, suppressPrint, costInterval)\n",
    "    \n",
    "    #will apply the gradient with stepSize where the gradient is in form \n",
    "    #given by the backProp function\n",
    "    def applyGradient(self, gradient, stepSize):\n",
    "        for i in range(1, len(gradient[0])): #for each matrix in the weight update, \n",
    "            #first value is None for convenience in indexing\n",
    "            self.weights[i] -= stepSize * gradient[0][i]\n",
    "        for i in range(1, len(gradient[1])): #for each vector in the bias update\n",
    "            self.bias[i] -= stepSize * gradient[1][i]\n",
    "    \n",
    "    #computes the correct and incorrect number of classified data points on data\n",
    "    #prints the number correct, number incorrect, percent correct, and percent incorrect\n",
    "    #returns a touple of the number correct and total number of data points\n",
    "    def validation(self, data, suppressPrint = False):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        if len(images) != len(labels):\n",
    "            raise NameError(\"improper input\")\n",
    "        correct = 0 \n",
    "        wrong = 0\n",
    "        for i in range(len(images)):\n",
    "            if (Neural_Network.classification(self.forwardProp(images[i]))) == labels[i]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        if not suppressPrint:\n",
    "            print(\"correct: \", correct)\n",
    "            print(\"wrong: \", wrong)\n",
    "            print(\"accuracy:\", 100.0*correct/len(images), \"%\")\n",
    "            print(\"error:\", 100.0*wrong/len(images), \"%\")\n",
    "        return (correct, len(images))\n",
    "    \n",
    "    #chooses a random batch of size batchSize from images and labels\n",
    "    #note it does not replace when sampling\n",
    "    def randomBatch(images, labels, batchSize):\n",
    "        indices = np.random.choice(len(images), batchSize, replace = False)\n",
    "        #if you set replace to True this will break the gradientDescent function\n",
    "        imageBatch = []\n",
    "        labelBatch = []\n",
    "        for i in range(len(indices)):\n",
    "            imageBatch.append(images[indices[i]])\n",
    "            labelBatch.append(labels[indices[i]])\n",
    "        return (imageBatch, labelBatch)\n",
    "        \n",
    "    #using backProp this will perform stoachastic gradient descent from the current initialization of the weights\n",
    "    #and biases on the given data. will choose batchSize many samples from data \n",
    "    #data in form touple of array of images and array of labels\n",
    "    #will stop after iterations many iterations. returns a touple of the best weights and biases\n",
    "    #and a list of the costs over time. will update the weights and biases in the neural network\n",
    "    #suppressPrint will not print anything between steps if set to true\n",
    "    #costInterval determines how often the cost will be re-computed, saved, printed (if suppressPrint is false)\n",
    "    #and how often the best weights and biases are saved. when set to 1 it occurs on every step\n",
    "    def stochasticGradientDescent(self, data, batchSize, iterations, learningRate, suppressPrint = False, costInterval = 5):\n",
    "        start = time.time()\n",
    "        bestCost = sys.maxsize\n",
    "        bestWandB = []\n",
    "        costs = []\n",
    "        accuracies = []\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        for j in range(iterations):\n",
    "            imageSet, labelSet = Neural_Network.randomBatch(images, labels, batchSize)\n",
    "            target = np.zeros(10)\n",
    "            gradients = []\n",
    "            for i in range(len(imageSet)):\n",
    "                target[labelSet[i]] = 1\n",
    "                output = self.forwardProp(imageSet[i])\n",
    "                gradients.append(self.backProp(target))\n",
    "                target[labelSet[i]] = 0\n",
    "            self.applyGradient(self.averageGradient(gradients), learningRate)\n",
    "            if j % costInterval == 0: #performs this many gradient steps before re-computing the cost for all examples\n",
    "                #this is somewhat of a hyper-parameter. Higher number = faster runtime but less training data, \n",
    "                #less info when it is running about how it is doing, and fewer opportunities to save the best configuration\n",
    "                cost = self.totalCost(data, self.costFunction)\n",
    "                costs.append(cost)\n",
    "                a = self.validation(data, suppressPrint)\n",
    "                accuracies.append(a[0]/a[1])\n",
    "                if not suppressPrint:\n",
    "                    print(cost)\n",
    "                if cost <= bestCost:#this saves the best configuration so far but only on steps when the cost is computed\n",
    "                    bestCost = cost\n",
    "                    bestWandB = (self.weights, self.bias)\n",
    "        trainingData = (costs, accuracies)\n",
    "        end = time.time()\n",
    "        print(iterations, \"iterations took\", end - start, \"seconds.\")\n",
    "        return (bestWandB, trainingData)\n",
    "    \n",
    "    #given a list of touples of the gradient in the form given by backProp\n",
    "    #will return the average gradient\n",
    "    def averageGradient(self, gradients):\n",
    "        averageGrad = (Neural_Network.constructWeights(self.shape), Neural_Network.constructBias(self.shape))\n",
    "        for i in range(len(averageGrad)): #weights then biases\n",
    "            for j in range(len(gradients)): #grad from each sample\n",
    "                for k in range(1, len(gradients[j][i])): #grad for each matrix\n",
    "                    averageGrad[i][k] += gradients[j][i][k]\n",
    "            for k in range(1,len(averageGrad[i])):#done at end to prevent rounding small numbers to zero\n",
    "                averageGrad[i][k] /= len(gradients) #if overflow is a problem then divide at each step\n",
    "        return averageGrad\n",
    "    \n",
    "    #this will return the gradient of the cost on the single target\n",
    "    #the form is a touple with the weights and then the biases\n",
    "    #in the same format as the weights and biases attributes for the Neural_Network class\n",
    "    def backProp(self, target):\n",
    "        if Neural_Network.inputChecks and len(target) != 10:\n",
    "            raise NameError(\"improper input\")\n",
    "        biasGrad = [None for _ in range(len(self.shape))]\n",
    "        weightGrad = [None for _ in range(len(self.shape))]\n",
    "        delta = self.costDeriv(self.activations[-1], target) * self.lastActPrime(self.zs[-1])\n",
    "        biasGrad[-1] = delta\n",
    "        weightGrad[-1] = np.tile(np.array([delta]).transpose(), (1,self.shape[-2]))*np.tile(np.array(self.activations[-2]),(self.shape[-1],1))\n",
    "        #this line (and the version of it below can be a little confusing, see readme)\n",
    "        for l in range(2, len(self.shape)):\n",
    "            z = self.zs[-l]\n",
    "            actDeriv = self.activationPrime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * actDeriv\n",
    "            biasGrad[-l] = delta\n",
    "            weightGrad[-l] = np.tile(np.array([delta]).transpose(), (1,self.shape[-l-1]))*np.tile(np.array(self.activations[-l-1]),(self.shape[-l],1))\n",
    "        return (weightGrad, biasGrad)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line right before the for loop in ```backProp()``` is a little confusing so I'll explain. $\\frac{\\partial C}{\\partial W^{l}_{j,k}} = a^{l-1}_{k} \\Delta^{l}_{j}$ where $C$ is the cost and $W^{l}_{j,k}$ is the weight going from the $k$th neuron in the $l-1$th layer to the $j$th neuron in the $l$th layer, $a^{l-1}_{k}$ is the activation on the $k$th neuron in the $l-1$th layer, and $\\Delta^{l}_{j}$ is the derivative of the cost with respect to the $j$th component of $z^{l}$. \n",
    "\n",
    "If you've never seen this before, it can be confusing so draw out a simple NN with very few neurons and write out the gradient of the weight matrix. You will notice that if you make a matrix with the same dimensions as $W^{l}$ where the columns are the vector $\\Delta^{l}$ and the same dimension matrix where the rows are the row vector $(a^{l-1})^{T}$ and do element-wise multiplication (not matrix multiplication) of these matrices, then the resulting matrix is the gradient of the cost with respect to the weight matrix $W^{l}$.\n",
    "\n",
    "If you're more comfortable with index notation you can also just see that in the gradient matrix of the cost with respect to the weight $W^{l}$ weights with the same row have the same $\\Delta$ value being multiplied. Similarly weights with the same column have the same $a^{l-1}$ value being multiplied.\n",
    "\n",
    "```np.tile``` allows you to create matrices from repeating column or row vectors. The [NumPy documentation](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.tile.html) explains it much better than I can. Thus in this line I use ```np.tile``` to create a matrix where the columns are the vector $\\Delta^{l}$ and a matrix where the rows are $(a^{l-1})^{T}$ and then perform element-wise multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shallowModel = Neural_Network(relu, reluPrime, softMaxCross, softMaxCrossPrime, shape = [28**2, 100, 10])\n",
    "deepModel = Neural_Network(relu, reluPrime, softMaxCross, softMaxCrossPrime, shape = [28**2, 300, 100, 10])\n",
    "\n",
    "with open('training_data.pickle', 'rb') as handle: #load previously trained models\n",
    "    lastData = pickle.load(handle)\n",
    "    \n",
    "shallowModel.weights, shallowModel.bias  = lastData[\"a\"]\n",
    "deepModel.weights, deepModel.bias = lastData[\"b\"]\n",
    "\n",
    "#accuracy results\n",
    "shallowModel.validation(valid_set)\n",
    "deepModel.validation(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
