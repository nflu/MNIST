{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ..., 8 4 8]\n"
     ]
    }
   ],
   "source": [
    "import _pickle as cPickle\n",
    "import gzip \n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f,encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "print(train_set[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#these are just various mathematical tools needed for the NN\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "    return expit(x)*(1-expit(x)) #use expit to prevent overflow with large values\n",
    "\n",
    "def sigmoid(x):\n",
    "    return expit(x) #admittedly this is a little unnecessary but I think it makes sense to have \n",
    "    #sigmoid and sigmoidPrime instead of expit and sigmoidPrime\n",
    "\n",
    "def relu(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def reluPrime(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def elementWise(f, x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = f(x[i])\n",
    "    return x    \n",
    "\n",
    "def softMax(x):\n",
    "    z = np.exp(x)\n",
    "    return z/sum(z)\n",
    "\n",
    "def softMaxPrime(x):\n",
    "    z = np.exp(x)\n",
    "    c = sum(z)\n",
    "    for i in range(len(z)):\n",
    "        z[i] = (c-z[i])*z[i]/(c**2)\n",
    "    return z\n",
    "\n",
    "def display(x, label, act):\n",
    "    strn = \"\"\n",
    "    for i in range(len(x)):\n",
    "        if i%28==0:\n",
    "            print(strn)\n",
    "            strn = \"\"\n",
    "        if x[i]==0:\n",
    "            strn += \" \"\n",
    "        if x[i]>=act:\n",
    "            strn += \"x\"\n",
    "    print(label)\n",
    "            \n",
    "def logLoss(x, target):\n",
    "    loss = 0\n",
    "    for i in range(len(x)):\n",
    "        loss += target[i]*np.log(x[i])+(1-target[i])*np.log(1-x[i])\n",
    "    return (-1.0/len(x))*loss\n",
    "    \n",
    "def logLossPrime(x, target):\n",
    "    grad = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        grad[i]=(-1.0/len(x))*(target[i]/x[i]-(1-target[i])/(1-x[i]))\n",
    "    return grad\n",
    "\n",
    "def MSE(x, target):\n",
    "    a = x-target\n",
    "    return np.dot(a,a)\n",
    "\n",
    "def MSEPrime(x, target):\n",
    "    return 2*(x-target)\n",
    "\n",
    "def crossEntropy(output, target):\n",
    "    cost = 0\n",
    "    for i in range(len(target)):\n",
    "        cost -= target[i]*np.log(output[i])\n",
    "    return cost\n",
    "\n",
    "def trainingGraph(trainingData):\n",
    "    costs = trainingData[0]\n",
    "    accuracies = trainingData[1]\n",
    "    plt.plot(costs, 'ro')\n",
    "    plt.ylabel(\"total cost\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.show()\n",
    "    plt.plot(100.0*(1.0-accuracies), 'bo')\n",
    "    plt.plot(\"total error percentage\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "class Neural_Network:\n",
    "    defaultSize = 16 #default number of neurons in hidden layers if no shape list given\n",
    "    inputChecks = True #this will change whether inputs that match the MNIST format are given\n",
    "    #can be turned off to allow for debugging on smaller examples\n",
    "    \n",
    "    #activation must be an activation function that works for a single scalar \n",
    "    #and activation prime is its derivative. costFunction is a cost function for a vector representing an output layer\n",
    "    #and the target vector. costDeriv is its derivative with respect to the vector of activations.\n",
    "    #shape is list of integers where shape[i] = the number of neurons in ith layer\n",
    "    #layers is an integer describing the number of layers\n",
    "    #shape and layers are optional. if both are given the shape list will be followed, if neither are given\n",
    "    #it will use the default value for layers and make all hidden layers have defaultSize many neurons\n",
    "    def __init__(self, activation, activationPrime, costFunction, costDeriv, shape = None, layers = 4):\n",
    "        if layers<2:\n",
    "            raise NameError(\"Too few layers. Need an input layer and an output layer.\")\n",
    "        if Neural_Network.inputChecks and (shape[0] != 28**2 or shape[len(shape)-1] != 10):\n",
    "            raise NameError(\"Improper input or output layer size. \\\n",
    "                            Must be 28^2 input neurons and 10 output neurons to work with MNIST.\")\n",
    "        if shape==None:\n",
    "            shape = [28**2] + [defaultSize for i in range(layers-2)] + [10]\n",
    "        self.activation = activation\n",
    "        self.activationPrime = activationPrime\n",
    "        self.shape = shape\n",
    "        self.costFunction = costFunction\n",
    "        self.costDeriv = costDeriv\n",
    "        self.weights = Neural_Network.constructWeights(shape) \n",
    "        #weights[i] is the weights going into layer i\n",
    "        self.bias = Neural_Network.constructBias(shape)\n",
    "        #bias[i] is the bias on layer i \n",
    "        self.activations = []\n",
    "        self.zs = []\n",
    "        \n",
    "    #returns a list of matrices of weights. weights[i] is the set of weights going into ith layer\n",
    "    #weights[i][j][k] represents the weight going from the kth neuron in layer i-1 to jth neuron in layer i \n",
    "    def constructWeights(shape):\n",
    "        weights = [None]\n",
    "        for i in range(len(shape)-1): \n",
    "            weights.append(np.random.uniform(-1,1,shape[i]*shape[i+1]).reshape((shape[i+1],shape[i])))\n",
    "        return weights\n",
    "    \n",
    "    #returns a list of vectors of biases. bias[i] is the set of biases on the ith layer\n",
    "    #bias[i][j] is the bias in the ith layer on the jth neuron\n",
    "    def constructBias(shape):\n",
    "        bias = [None]\n",
    "        for i in range(1,len(shape)): \n",
    "            bias.append(np.random.uniform(-1,1,shape[i]))\n",
    "        return bias\n",
    "    \n",
    "    #performs forward propagation on the input x with the current weights and biases\n",
    "    #using activation function from the constructor returns the activations on the last layer \n",
    "    #updates the zs and activations attributes\n",
    "    def forwardProp(self, x):\n",
    "        if Neural_Network.inputChecks and len(x) != 28**2:\n",
    "            raise NameError(\"improper input\")\n",
    "        prevAct = x\n",
    "        act = []\n",
    "        self.activations = []\n",
    "        self.zs = []\n",
    "        self.activations.append(prevAct)\n",
    "        for i in range(1,len(self.shape)): #each layer\n",
    "            z = np.dot(self.weights[i], prevAct) + self.bias[i]\n",
    "            act = elementWise(self.activation, z)\n",
    "            self.activations.append(act)\n",
    "            self.zs.append(z)\n",
    "            prevAct = act\n",
    "        return act\n",
    "    \n",
    "    #returns the total cost for the neural network on all datapoints in data\n",
    "    #using cost function costFunc for each point. returns cost as a scalar\n",
    "    def totalCost(self, data, costFunc):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        if Neural_Network.inputChecks and len(images) != len(labels):\n",
    "            raise NameError(\"improper input\")\n",
    "        target = np.zeros(10)\n",
    "        cost = 0 \n",
    "        for i in range(len(images)):\n",
    "            target[labels[i]] = 1\n",
    "            out = self.forwardProp(images[i])\n",
    "            cost += costFunc(out, target)\n",
    "            target[labels[i]] = 0\n",
    "        return cost\n",
    "        \n",
    "    #returns the classification as a scalar based on the outputActivations\n",
    "    #picks the index with highest activation\n",
    "    def classification(outputActivations):\n",
    "        maximum = -1\n",
    "        index = -1\n",
    "        for i in range(len(outputActivations)):\n",
    "            if outputActivations[i] >= maximum:\n",
    "                index = i\n",
    "                maximum = outputActivations[i]\n",
    "        return index\n",
    "     \n",
    "    #randomly initializes all weights and biases    \n",
    "    def randomInitialization(self):\n",
    "        self.weights = Neural_Network.constructWeights(self.shape) \n",
    "        self.bias = Neural_Network.constructBias(self.shape)\n",
    "    \n",
    "    #using backProp this will perform gradient descent from the current initialization of the weights\n",
    "    #and biases on the given data. data in form touple of array of images and array of labels\n",
    "    #will stop after iterations many iterations. returns a touple of the best weights and biases\n",
    "    #and a list of the costs over time. will update the weights and biases in the neural network\n",
    "    def gradientDescent(self, data, iterations, learningRate):\n",
    "        return self.stochasticGradientDescent(data, len(data[0]), iterations, learningRate)\n",
    "    \n",
    "    #will apply the gradient with stepSize where the gradient is in form \n",
    "    #given by the backProp function\n",
    "    def applyGradient(self, gradient, stepSize):\n",
    "        for i in range(1, len(gradient[0])): #for each matrix in the weight update, \n",
    "            #first value is None for convenience in indexing\n",
    "            self.weights[i] -= stepSize * gradient[0][i]\n",
    "        for i in range(1, len(gradient[1])): #for each vector in the bias update\n",
    "            self.bias[i] -= stepSize * gradient[1][i]\n",
    "    \n",
    "    #computes the correct and incorrect number of classified data points on data\n",
    "    #prints the number correct, number incorrect, percent correct, and percent incorrect\n",
    "    #returns a touple of the number correct and total number of data points\n",
    "    def validation(self, data):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        if len(images) != len(labels):\n",
    "            raise NameError(\"improper input\")\n",
    "        correct = 0 \n",
    "        wrong = 0\n",
    "        for i in range(len(images)):\n",
    "            if (Neural_Network.classification(self.forwardProp(images[i]))) == labels[i]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        print(\"correct: \", correct)\n",
    "        print(\"wrong: \", wrong)\n",
    "        print(\"accuracy:\", 100.0*correct/len(images), \"%\")\n",
    "        print(\"error:\", 100.0*wrong/len(images), \"%\")\n",
    "        return (correct, len(images))\n",
    "    \n",
    "    #chooses a random batch of size batchSize from images and labels\n",
    "    #note it does not replace when sampling\n",
    "    def randomBatch(images, labels, batchSize):\n",
    "        indices = np.random.choice(len(images), batchSize, replace = False)\n",
    "        #if you set replace to True this will break the gradientDescent function\n",
    "        imageBatch = []\n",
    "        labelBatch = []\n",
    "        for i in range(len(indices)):\n",
    "            imageBatch.append(images[indices[i]])\n",
    "            labelBatch.append(labels[indices[i]])\n",
    "        return (imageBatch, labelBatch)\n",
    "        \n",
    "    #using backProp this will perform stoachastic gradient descent from the current initialization of the weights\n",
    "    #and biases on the given data. will choose batchSize many samples from data \n",
    "    #data in form touple of array of images and array of labels\n",
    "    #will stop after iterations many iterations. returns a touple of the best weights and biases\n",
    "    #and a list of the costs over time. will update the weights and biases in the neural network\n",
    "    def stochasticGradientDescent(self, data, batchSize, iterations, learningRate):\n",
    "        start = time.time()\n",
    "        bestCost = sys.maxsize\n",
    "        bestWandB = []\n",
    "        costs = []\n",
    "        accuracies = []\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        for _ in range(iterations):\n",
    "            for i in range(5): #performs 5 gradient steps before re-computing the cost for all examples\n",
    "                #this is somewhat of a hyper-parameter that just allows things to run pretty fast\n",
    "                imageSet, labelSet = Neural_Network.randomBatch(images, labels, batchSize)\n",
    "                target = np.zeros(10)\n",
    "                gradients = []\n",
    "                for i in range(len(imageSet)):\n",
    "                    target[labelSet[i]] = 1\n",
    "                    output = self.forwardProp(imageSet[i])\n",
    "                    gradients.append(self.backProp(target))\n",
    "                    target[labelSet[i]] = 0\n",
    "                self.applyGradient(self.averageGradient(gradients), learningRate)\n",
    "            cost = self.totalCost(data, self.costFunction)\n",
    "            costs.append(cost)\n",
    "            a = self.validation(data)\n",
    "            accuracies.append(a[0]/a[1])\n",
    "            print(cost)\n",
    "            if cost <= bestCost:\n",
    "                bestCost = cost\n",
    "                bestWandB = (self.weights, self.bias)\n",
    "        trainingData = (costs, accuracies)\n",
    "        end = time.time()\n",
    "        print(iterations, \"iterations took\", end - start, \"seconds.\")\n",
    "        return (bestWandB, trainingData)\n",
    "    \n",
    "    #given a list of touples of the gradient in the form given by backProp\n",
    "    #will return the average gradient\n",
    "    def averageGradient(self, gradients):\n",
    "        averageGrad = (Neural_Network.constructWeights(self.shape), Neural_Network.constructBias(self.shape))\n",
    "        for i in range(len(averageGrad)): #weights then biases\n",
    "            for j in range(len(gradients)): #grad from each sample\n",
    "                for k in range(1, len(gradients[j][i])): #grad for each matrix\n",
    "                    averageGrad[i][k] += gradients[j][i][k]\n",
    "            for k in range(1,len(averageGrad[i])):#done at end to prevent rounding small numbers to zero\n",
    "                averageGrad[i][k] /= len(gradients) #if overflow is a problem then divide at each step\n",
    "        return averageGrad\n",
    "    \n",
    "    #this will return the gradient of the cost on the single target\n",
    "    #the form is a touple with the weights and then the biases\n",
    "    #in the same format as the weights and biases attributes for the Neural_Network class\n",
    "    def backProp(self, target):\n",
    "        if Neural_Network.inputChecks and len(target) != 10:\n",
    "            raise NameError(\"improper input\")\n",
    "        biasGrad = [None for _ in range(len(self.shape))]\n",
    "        weightGrad = [None for _ in range(len(self.shape))]\n",
    "        delta = self.costDeriv(self.activations[-1], target) * self.activationPrime(self.zs[-1])\n",
    "        biasGrad[-1] = delta\n",
    "        weightGrad[-1] = np.tile(np.array([delta]).transpose(), (1,self.shape[-2]))*np.tile(np.array(self.activations[-2]),(self.shape[-1],1))\n",
    "        #this line (and the version of it below can be a little confusing, see readme)\n",
    "        for l in range(2, len(self.shape)):\n",
    "            z = self.zs[-l]\n",
    "            actDeriv = self.activationPrime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * actDeriv\n",
    "            biasGrad[-l] = delta\n",
    "            weightGrad[-l] = np.tile(np.array([delta]).transpose(), (1,self.shape[-l-1]))*np.tile(np.array(self.activations[-l-1]),(self.shape[-l],1))\n",
    "        return (weightGrad, biasGrad)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line right before the for loop in ```backProp()``` is a little confusing so I'll explain. $\\frac{\\partial C}{\\partial W^{l}_{j,k}} = a^{l-1}_{k} \\Delta^{l}_{j}$ where $C$ is the cost and $W^{l}_{j,k}$ is the weight going from the $k$th neuron in the $l-1$th layer to the $j$th neuron in the $l$th layer, $a^{l-1}_{k}$ is the activation on the $k$th neuron in the $l-1$th layer, and  $\\Delta^{l}_{j}$ is the derivative of the cost with respect to the $j$th component of $z^{l}$. If you've never seen this before, it can be confusing so draw out a simple NN with very few neurons and write out the gradient of the weight matrix. You will notice that if you make a matrix with the same dimensions as $W^{l}$ where the columns are the vector $\\Delta^{l}$ and the same dimension matrix where the rows are the row vector $(a^{l-1})^{T}$ and do element-wise multiplication (not matrix multiplication) of these matrices, then the resulting matrix is the gradient of the cost with respect to the weight matrix $W^{l}$.\n",
    "\n",
    "```np.tile``` allows you to create matrices from repeating column or row vectors. The [NumPy documentation](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.tile.html) explains it much better than I can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  11104\n",
      "wrong:  38896\n",
      "accuracy: 22.208 %\n",
      "error: 77.792 %\n",
      "44789.5725365\n",
      "correct:  14005\n",
      "wrong:  35995\n",
      "accuracy: 28.01 %\n",
      "error: 71.99 %\n",
      "43584.0692002\n",
      "correct:  22897\n",
      "wrong:  27103\n",
      "accuracy: 45.794 %\n",
      "error: 54.206 %\n",
      "40019.6974792\n",
      "correct:  23590\n",
      "wrong:  26410\n",
      "accuracy: 47.18 %\n",
      "error: 52.82 %\n",
      "38143.7754353\n",
      "correct:  29943\n",
      "wrong:  20057\n",
      "accuracy: 59.886 %\n",
      "error: 40.114 %\n",
      "33246.2197922\n",
      "correct:  30307\n",
      "wrong:  19693\n",
      "accuracy: 60.614 %\n",
      "error: 39.386 %\n",
      "31592.0676809\n",
      "correct:  33231\n",
      "wrong:  16769\n",
      "accuracy: 66.462 %\n",
      "error: 33.538 %\n",
      "28148.3615751\n",
      "correct:  34328\n",
      "wrong:  15672\n",
      "accuracy: 68.656 %\n",
      "error: 31.344 %\n",
      "27126.4580038\n",
      "correct:  34458\n",
      "wrong:  15542\n",
      "accuracy: 68.916 %\n",
      "error: 31.084 %\n",
      "25714.3637737\n",
      "correct:  35422\n",
      "wrong:  14578\n",
      "accuracy: 70.844 %\n",
      "error: 29.156 %\n",
      "24907.8291787\n",
      "correct:  35556\n",
      "wrong:  14444\n",
      "accuracy: 71.112 %\n",
      "error: 28.888 %\n",
      "23749.8453958\n",
      "correct:  36243\n",
      "wrong:  13757\n",
      "accuracy: 72.486 %\n",
      "error: 27.514 %\n",
      "22947.8592803\n",
      "correct:  36624\n",
      "wrong:  13376\n",
      "accuracy: 73.248 %\n",
      "error: 26.752 %\n",
      "22584.1599972\n",
      "correct:  37131\n",
      "wrong:  12869\n",
      "accuracy: 74.262 %\n",
      "error: 25.738 %\n",
      "21750.4409565\n",
      "correct:  37206\n",
      "wrong:  12794\n",
      "accuracy: 74.412 %\n",
      "error: 25.588 %\n",
      "21396.6575347\n",
      "correct:  37582\n",
      "wrong:  12418\n",
      "accuracy: 75.164 %\n",
      "error: 24.836 %\n",
      "21055.5420719\n",
      "correct:  37321\n",
      "wrong:  12679\n",
      "accuracy: 74.642 %\n",
      "error: 25.358 %\n",
      "20738.8960583\n",
      "correct:  37535\n",
      "wrong:  12465\n",
      "accuracy: 75.07 %\n",
      "error: 24.93 %\n",
      "20560.7199084\n",
      "correct:  37732\n",
      "wrong:  12268\n",
      "accuracy: 75.464 %\n",
      "error: 24.536 %\n",
      "20122.563032\n",
      "correct:  37599\n",
      "wrong:  12401\n",
      "accuracy: 75.198 %\n",
      "error: 24.802 %\n",
      "19845.0754134\n",
      "correct:  37824\n",
      "wrong:  12176\n",
      "accuracy: 75.648 %\n",
      "error: 24.352 %\n",
      "19506.2626198\n",
      "correct:  37616\n",
      "wrong:  12384\n",
      "accuracy: 75.232 %\n",
      "error: 24.768 %\n",
      "19424.3861323\n",
      "correct:  37940\n",
      "wrong:  12060\n",
      "accuracy: 75.88 %\n",
      "error: 24.12 %\n",
      "19172.3133465\n",
      "correct:  38097\n",
      "wrong:  11903\n",
      "accuracy: 76.194 %\n",
      "error: 23.806 %\n",
      "18765.8144209\n",
      "correct:  38079\n",
      "wrong:  11921\n",
      "accuracy: 76.158 %\n",
      "error: 23.842 %\n",
      "18690.2848611\n",
      "correct:  38359\n",
      "wrong:  11641\n",
      "accuracy: 76.718 %\n",
      "error: 23.282 %\n",
      "18399.3286387\n",
      "correct:  38291\n",
      "wrong:  11709\n",
      "accuracy: 76.582 %\n",
      "error: 23.418 %\n",
      "18294.6153215\n",
      "correct:  37874\n",
      "wrong:  12126\n",
      "accuracy: 75.748 %\n",
      "error: 24.252 %\n",
      "18442.054921\n",
      "correct:  38193\n",
      "wrong:  11807\n",
      "accuracy: 76.386 %\n",
      "error: 23.614 %\n",
      "18001.5335806\n",
      "correct:  38514\n",
      "wrong:  11486\n",
      "accuracy: 77.028 %\n",
      "error: 22.972 %\n",
      "17739.3134676\n",
      "correct:  38847\n",
      "wrong:  11153\n",
      "accuracy: 77.694 %\n",
      "error: 22.306 %\n",
      "17321.0042556\n",
      "correct:  38886\n",
      "wrong:  11114\n",
      "accuracy: 77.772 %\n",
      "error: 22.228 %\n",
      "17116.5067107\n",
      "correct:  39158\n",
      "wrong:  10842\n",
      "accuracy: 78.316 %\n",
      "error: 21.684 %\n",
      "17054.9647907\n",
      "correct:  39425\n",
      "wrong:  10575\n",
      "accuracy: 78.85 %\n",
      "error: 21.15 %\n",
      "16703.6524785\n",
      "correct:  39607\n",
      "wrong:  10393\n",
      "accuracy: 79.214 %\n",
      "error: 20.786 %\n",
      "16552.8507536\n",
      "correct:  39611\n",
      "wrong:  10389\n",
      "accuracy: 79.222 %\n",
      "error: 20.778 %\n",
      "16340.1707643\n",
      "correct:  39437\n",
      "wrong:  10563\n",
      "accuracy: 78.874 %\n",
      "error: 21.126 %\n",
      "16341.9453543\n",
      "correct:  39661\n",
      "wrong:  10339\n",
      "accuracy: 79.322 %\n",
      "error: 20.678 %\n",
      "16127.2381855\n",
      "correct:  39621\n",
      "wrong:  10379\n",
      "accuracy: 79.242 %\n",
      "error: 20.758 %\n",
      "15982.7898287\n",
      "correct:  40070\n",
      "wrong:  9930\n",
      "accuracy: 80.14 %\n",
      "error: 19.86 %\n",
      "15721.5571614\n",
      "40 iterations took 657.8084588050842 seconds.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Neural_Network' object has no attribute 'validate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-4e61bd41c0c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeural_Network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoidPrime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSEPrime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochasticGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Neural_Network' object has no attribute 'validate'"
     ]
    }
   ],
   "source": [
    "b = Neural_Network(expit, sigmoidPrime, MSE, MSEPrime, shape=[28**2, 30, 30, 10])\n",
    "train1 = b.stochasticGradientDescent(train_set, 2000, 40, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  8231\n",
      "wrong:  1769\n",
      "accuracy: 82.31 %\n",
      "error: 17.69 %\n",
      "correct:  8118\n",
      "wrong:  1882\n",
      "accuracy: 81.18 %\n",
      "error: 18.82 %\n",
      "correct:  40065\n",
      "wrong:  9935\n",
      "accuracy: 80.13 %\n",
      "error: 19.87 %\n",
      "15719.9204202\n",
      "correct:  40058\n",
      "wrong:  9942\n",
      "accuracy: 80.116 %\n",
      "error: 19.884 %\n",
      "15718.9583885\n",
      "correct:  39993\n",
      "wrong:  10007\n",
      "accuracy: 79.986 %\n",
      "error: 20.014 %\n",
      "15718.4445488\n",
      "correct:  39975\n",
      "wrong:  10025\n",
      "accuracy: 79.95 %\n",
      "error: 20.05 %\n",
      "15718.2595221\n",
      "correct:  39970\n",
      "wrong:  10030\n",
      "accuracy: 79.94 %\n",
      "error: 20.06 %\n",
      "15718.262981\n",
      "correct:  39955\n",
      "wrong:  10045\n",
      "accuracy: 79.91 %\n",
      "error: 20.09 %\n",
      "15718.324262\n",
      "correct:  39970\n",
      "wrong:  10030\n",
      "accuracy: 79.94 %\n",
      "error: 20.06 %\n",
      "15718.325023\n",
      "correct:  39970\n",
      "wrong:  10030\n",
      "accuracy: 79.94 %\n",
      "error: 20.06 %\n",
      "15718.1790087\n",
      "correct:  39968\n",
      "wrong:  10032\n",
      "accuracy: 79.936 %\n",
      "error: 20.064 %\n",
      "15717.8325462\n",
      "correct:  39957\n",
      "wrong:  10043\n",
      "accuracy: 79.914 %\n",
      "error: 20.086 %\n",
      "15717.2756343\n",
      "correct:  39960\n",
      "wrong:  10040\n",
      "accuracy: 79.92 %\n",
      "error: 20.08 %\n",
      "15716.4980976\n",
      "correct:  39955\n",
      "wrong:  10045\n",
      "accuracy: 79.91 %\n",
      "error: 20.09 %\n",
      "15715.5093384\n",
      "correct:  39954\n",
      "wrong:  10046\n",
      "accuracy: 79.908 %\n",
      "error: 20.092 %\n",
      "15714.3343108\n",
      "correct:  39954\n",
      "wrong:  10046\n",
      "accuracy: 79.908 %\n",
      "error: 20.092 %\n",
      "15712.9796242\n",
      "correct:  39954\n",
      "wrong:  10046\n",
      "accuracy: 79.908 %\n",
      "error: 20.092 %\n",
      "15711.456847\n",
      "correct:  39952\n",
      "wrong:  10048\n",
      "accuracy: 79.904 %\n",
      "error: 20.096 %\n",
      "15709.7887255\n",
      "correct:  39952\n",
      "wrong:  10048\n",
      "accuracy: 79.904 %\n",
      "error: 20.096 %\n",
      "15707.9774516\n",
      "correct:  39951\n",
      "wrong:  10049\n",
      "accuracy: 79.902 %\n",
      "error: 20.098 %\n",
      "15706.0398636\n",
      "correct:  39946\n",
      "wrong:  10054\n",
      "accuracy: 79.892 %\n",
      "error: 20.108 %\n",
      "15703.9838156\n",
      "correct:  39938\n",
      "wrong:  10062\n",
      "accuracy: 79.876 %\n",
      "error: 20.124 %\n",
      "15701.8205022\n",
      "correct:  39932\n",
      "wrong:  10068\n",
      "accuracy: 79.864 %\n",
      "error: 20.136 %\n",
      "15699.5626527\n",
      "correct:  39930\n",
      "wrong:  10070\n",
      "accuracy: 79.86 %\n",
      "error: 20.14 %\n",
      "15697.2146983\n",
      "correct:  39927\n",
      "wrong:  10073\n",
      "accuracy: 79.854 %\n",
      "error: 20.146 %\n",
      "15694.782326\n",
      "correct:  39929\n",
      "wrong:  10071\n",
      "accuracy: 79.858 %\n",
      "error: 20.142 %\n",
      "15692.2783577\n",
      "correct:  39934\n",
      "wrong:  10066\n",
      "accuracy: 79.868 %\n",
      "error: 20.132 %\n",
      "15689.7048602\n",
      "correct:  39936\n",
      "wrong:  10064\n",
      "accuracy: 79.872 %\n",
      "error: 20.128 %\n",
      "15687.06736\n",
      "correct:  39939\n",
      "wrong:  10061\n",
      "accuracy: 79.878 %\n",
      "error: 20.122 %\n",
      "15684.3815251\n",
      "correct:  39942\n",
      "wrong:  10058\n",
      "accuracy: 79.884 %\n",
      "error: 20.116 %\n",
      "15681.6399618\n",
      "correct:  39942\n",
      "wrong:  10058\n",
      "accuracy: 79.884 %\n",
      "error: 20.116 %\n",
      "15678.8585932\n",
      "correct:  39936\n",
      "wrong:  10064\n",
      "accuracy: 79.872 %\n",
      "error: 20.128 %\n",
      "15676.0343237\n",
      "correct:  39937\n",
      "wrong:  10063\n",
      "accuracy: 79.874 %\n",
      "error: 20.126 %\n",
      "15673.1736924\n",
      "correct:  39939\n",
      "wrong:  10061\n",
      "accuracy: 79.878 %\n",
      "error: 20.122 %\n",
      "15670.2862104\n",
      "correct:  39941\n",
      "wrong:  10059\n",
      "accuracy: 79.882 %\n",
      "error: 20.118 %\n",
      "15667.3708685\n",
      "correct:  39941\n",
      "wrong:  10059\n",
      "accuracy: 79.882 %\n",
      "error: 20.118 %\n",
      "15664.4253421\n",
      "correct:  39949\n",
      "wrong:  10051\n",
      "accuracy: 79.898 %\n",
      "error: 20.102 %\n",
      "15661.46358\n",
      "correct:  39950\n",
      "wrong:  10050\n",
      "accuracy: 79.9 %\n",
      "error: 20.1 %\n",
      "15658.4794499\n",
      "correct:  39954\n",
      "wrong:  10046\n",
      "accuracy: 79.908 %\n",
      "error: 20.092 %\n",
      "15655.4808684\n",
      "correct:  39957\n",
      "wrong:  10043\n",
      "accuracy: 79.914 %\n",
      "error: 20.086 %\n",
      "15652.4695361\n",
      "correct:  39960\n",
      "wrong:  10040\n",
      "accuracy: 79.92 %\n",
      "error: 20.08 %\n",
      "15649.4480106\n",
      "correct:  39965\n",
      "wrong:  10035\n",
      "accuracy: 79.93 %\n",
      "error: 20.07 %\n",
      "15646.4155312\n",
      "correct:  39967\n",
      "wrong:  10033\n",
      "accuracy: 79.934 %\n",
      "error: 20.066 %\n",
      "15643.3766439\n",
      "correct:  39966\n",
      "wrong:  10034\n",
      "accuracy: 79.932 %\n",
      "error: 20.068 %\n",
      "15640.3329752\n",
      "correct:  39961\n",
      "wrong:  10039\n",
      "accuracy: 79.922 %\n",
      "error: 20.078 %\n",
      "15637.2861194\n",
      "correct:  39961\n",
      "wrong:  10039\n",
      "accuracy: 79.922 %\n",
      "error: 20.078 %\n",
      "15634.2427314\n",
      "correct:  39965\n",
      "wrong:  10035\n",
      "accuracy: 79.93 %\n",
      "error: 20.07 %\n",
      "15631.1950299\n",
      "correct:  39965\n",
      "wrong:  10035\n",
      "accuracy: 79.93 %\n",
      "error: 20.07 %\n",
      "15628.151173\n",
      "correct:  39963\n",
      "wrong:  10037\n",
      "accuracy: 79.926 %\n",
      "error: 20.074 %\n",
      "15625.1054524\n",
      "correct:  39966\n",
      "wrong:  10034\n",
      "accuracy: 79.932 %\n",
      "error: 20.068 %\n",
      "15622.069129\n",
      "correct:  39964\n",
      "wrong:  10036\n",
      "accuracy: 79.928 %\n",
      "error: 20.072 %\n",
      "15619.0331998\n",
      "correct:  39964\n",
      "wrong:  10036\n",
      "accuracy: 79.928 %\n",
      "error: 20.072 %\n",
      "15616.0049652\n",
      "correct:  39963\n",
      "wrong:  10037\n",
      "accuracy: 79.926 %\n",
      "error: 20.074 %\n",
      "15612.988188\n",
      "correct:  39961\n",
      "wrong:  10039\n",
      "accuracy: 79.922 %\n",
      "error: 20.078 %\n",
      "15609.9806317\n",
      "correct:  39964\n",
      "wrong:  10036\n",
      "accuracy: 79.928 %\n",
      "error: 20.072 %\n",
      "15606.9812852\n",
      "correct:  39967\n",
      "wrong:  10033\n",
      "accuracy: 79.934 %\n",
      "error: 20.066 %\n",
      "15603.9809443\n",
      "correct:  39966\n",
      "wrong:  10034\n",
      "accuracy: 79.932 %\n",
      "error: 20.068 %\n",
      "15600.9943519\n",
      "correct:  39966\n",
      "wrong:  10034\n",
      "accuracy: 79.932 %\n",
      "error: 20.068 %\n",
      "15598.0208359\n",
      "correct:  39963\n",
      "wrong:  10037\n",
      "accuracy: 79.926 %\n",
      "error: 20.074 %\n",
      "15595.0533632\n",
      "correct:  39966\n",
      "wrong:  10034\n",
      "accuracy: 79.932 %\n",
      "error: 20.068 %\n",
      "15592.0951588\n",
      "correct:  39970\n",
      "wrong:  10030\n",
      "accuracy: 79.94 %\n",
      "error: 20.06 %\n",
      "15589.1457133\n",
      "correct:  39967\n",
      "wrong:  10033\n",
      "accuracy: 79.934 %\n",
      "error: 20.066 %\n",
      "15586.207933\n",
      "correct:  39968\n",
      "wrong:  10032\n",
      "accuracy: 79.936 %\n",
      "error: 20.064 %\n",
      "15583.2785021\n",
      "correct:  39969\n",
      "wrong:  10031\n",
      "accuracy: 79.938 %\n",
      "error: 20.062 %\n",
      "15580.3531447\n",
      "correct:  39970\n",
      "wrong:  10030\n",
      "accuracy: 79.94 %\n",
      "error: 20.06 %\n",
      "15577.4396155\n",
      "correct:  39976\n",
      "wrong:  10024\n",
      "accuracy: 79.952 %\n",
      "error: 20.048 %\n",
      "15574.5301669\n",
      "correct:  39977\n",
      "wrong:  10023\n",
      "accuracy: 79.954 %\n",
      "error: 20.046 %\n",
      "15571.6237725\n",
      "correct:  39977\n",
      "wrong:  10023\n",
      "accuracy: 79.954 %\n",
      "error: 20.046 %\n",
      "15568.7300729\n",
      "correct:  39981\n",
      "wrong:  10019\n",
      "accuracy: 79.962 %\n",
      "error: 20.038 %\n",
      "15565.8403842\n",
      "correct:  39985\n",
      "wrong:  10015\n",
      "accuracy: 79.97 %\n",
      "error: 20.03 %\n",
      "15562.9569026\n",
      "correct:  39991\n",
      "wrong:  10009\n",
      "accuracy: 79.982 %\n",
      "error: 20.018 %\n",
      "15560.0706734\n",
      "correct:  39992\n",
      "wrong:  10008\n",
      "accuracy: 79.984 %\n",
      "error: 20.016 %\n",
      "15557.1933878\n",
      "correct:  39992\n",
      "wrong:  10008\n",
      "accuracy: 79.984 %\n",
      "error: 20.016 %\n",
      "15554.317806\n",
      "correct:  39992\n",
      "wrong:  10008\n",
      "accuracy: 79.984 %\n",
      "error: 20.016 %\n",
      "15551.4514288\n",
      "correct:  39994\n",
      "wrong:  10006\n",
      "accuracy: 79.988 %\n",
      "error: 20.012 %\n",
      "15548.5839122\n",
      "correct:  39994\n",
      "wrong:  10006\n",
      "accuracy: 79.988 %\n",
      "error: 20.012 %\n",
      "15545.7215181\n",
      "correct:  39994\n",
      "wrong:  10006\n",
      "accuracy: 79.988 %\n",
      "error: 20.012 %\n",
      "15542.8679245\n",
      "correct:  39999\n",
      "wrong:  10001\n",
      "accuracy: 79.998 %\n",
      "error: 20.002 %\n",
      "15540.021021\n",
      "correct:  40000\n",
      "wrong:  10000\n",
      "accuracy: 80.0 %\n",
      "error: 20.0 %\n",
      "15537.1731915\n",
      "correct:  39997\n",
      "wrong:  10003\n",
      "accuracy: 79.994 %\n",
      "error: 20.006 %\n",
      "15534.3346668\n",
      "correct:  39995\n",
      "wrong:  10005\n",
      "accuracy: 79.99 %\n",
      "error: 20.01 %\n",
      "15531.4991354\n",
      "correct:  39998\n",
      "wrong:  10002\n",
      "accuracy: 79.996 %\n",
      "error: 20.004 %\n",
      "15528.6684382\n",
      "correct:  39999\n",
      "wrong:  10001\n",
      "accuracy: 79.998 %\n",
      "error: 20.002 %\n",
      "15525.8445757\n",
      "correct:  39994\n",
      "wrong:  10006\n",
      "accuracy: 79.988 %\n",
      "error: 20.012 %\n",
      "15523.0264896\n",
      "correct:  39993\n",
      "wrong:  10007\n",
      "accuracy: 79.986 %\n",
      "error: 20.014 %\n",
      "15520.2153414\n",
      "correct:  39990\n",
      "wrong:  10010\n",
      "accuracy: 79.98 %\n",
      "error: 20.02 %\n",
      "15517.3993115\n",
      "correct:  39988\n",
      "wrong:  10012\n",
      "accuracy: 79.976 %\n",
      "error: 20.024 %\n",
      "15514.5968422\n",
      "correct:  39993\n",
      "wrong:  10007\n",
      "accuracy: 79.986 %\n",
      "error: 20.014 %\n",
      "15511.8050914\n",
      "correct:  39998\n",
      "wrong:  10002\n",
      "accuracy: 79.996 %\n",
      "error: 20.004 %\n",
      "15509.0143841\n",
      "correct:  39997\n",
      "wrong:  10003\n",
      "accuracy: 79.994 %\n",
      "error: 20.006 %\n",
      "15506.2308685\n",
      "correct:  40001\n",
      "wrong:  9999\n",
      "accuracy: 80.002 %\n",
      "error: 19.998 %\n",
      "15503.454952\n",
      "correct:  40006\n",
      "wrong:  9994\n",
      "accuracy: 80.012 %\n",
      "error: 19.988 %\n",
      "15500.6809458\n",
      "correct:  40006\n",
      "wrong:  9994\n",
      "accuracy: 80.012 %\n",
      "error: 19.988 %\n",
      "15497.9189118\n",
      "correct:  40008\n",
      "wrong:  9992\n",
      "accuracy: 80.016 %\n",
      "error: 19.984 %\n",
      "15495.1603566\n",
      "correct:  40012\n",
      "wrong:  9988\n",
      "accuracy: 80.024 %\n",
      "error: 19.976 %\n",
      "15492.4102876\n",
      "correct:  40017\n",
      "wrong:  9983\n",
      "accuracy: 80.034 %\n",
      "error: 19.966 %\n",
      "15489.6666653\n",
      "correct:  40013\n",
      "wrong:  9987\n",
      "accuracy: 80.026 %\n",
      "error: 19.974 %\n",
      "15486.9296822\n",
      "correct:  40011\n",
      "wrong:  9989\n",
      "accuracy: 80.022 %\n",
      "error: 19.978 %\n",
      "15484.1957973\n",
      "correct:  40013\n",
      "wrong:  9987\n",
      "accuracy: 80.026 %\n",
      "error: 19.974 %\n",
      "15481.4718091\n",
      "correct:  40015\n",
      "wrong:  9985\n",
      "accuracy: 80.03 %\n",
      "error: 19.97 %\n",
      "15478.7532706\n",
      "correct:  40016\n",
      "wrong:  9984\n",
      "accuracy: 80.032 %\n",
      "error: 19.968 %\n",
      "15476.0393304\n",
      "correct:  40019\n",
      "wrong:  9981\n",
      "accuracy: 80.038 %\n",
      "error: 19.962 %\n",
      "15473.3301095\n",
      "100 iterations took 11326.883607387543 seconds.\n",
      "correct:  8209\n",
      "wrong:  1791\n",
      "accuracy: 82.09 %\n",
      "error: 17.91 %\n",
      "correct:  8116\n",
      "wrong:  1884\n",
      "accuracy: 81.16 %\n",
      "error: 18.84 %\n",
      "correct:  40020\n",
      "wrong:  9980\n",
      "accuracy: 80.04 %\n",
      "error: 19.96 %\n",
      "15473.059828\n",
      "correct:  40019\n",
      "wrong:  9981\n",
      "accuracy: 80.038 %\n",
      "error: 19.962 %\n",
      "15472.7895988\n",
      "correct:  40019\n",
      "wrong:  9981\n",
      "accuracy: 80.038 %\n",
      "error: 19.962 %\n",
      "15472.5192704\n",
      "correct:  40019\n",
      "wrong:  9981\n",
      "accuracy: 80.038 %\n",
      "error: 19.962 %\n",
      "15472.2487402\n",
      "correct:  40019\n",
      "wrong:  9981\n",
      "accuracy: 80.038 %\n",
      "error: 19.962 %\n",
      "15471.9782339\n",
      "correct:  40020\n",
      "wrong:  9980\n",
      "accuracy: 80.04 %\n",
      "error: 19.96 %\n",
      "15471.7080043\n",
      "correct:  40020\n",
      "wrong:  9980\n",
      "accuracy: 80.04 %\n",
      "error: 19.96 %\n",
      "15471.4380573\n",
      "correct:  40020\n",
      "wrong:  9980\n",
      "accuracy: 80.04 %\n",
      "error: 19.96 %\n",
      "15471.1678712\n",
      "correct:  40020\n",
      "wrong:  9980\n",
      "accuracy: 80.04 %\n",
      "error: 19.96 %\n",
      "15470.8975629\n",
      "correct:  40019\n",
      "wrong:  9981\n",
      "accuracy: 80.038 %\n",
      "error: 19.962 %\n",
      "15470.6276251\n",
      "correct:  40019\n",
      "wrong:  9981\n",
      "accuracy: 80.038 %\n",
      "error: 19.962 %\n",
      "15470.3581402\n",
      "correct:  40018\n",
      "wrong:  9982\n",
      "accuracy: 80.036 %\n",
      "error: 19.964 %\n",
      "15470.0875695\n",
      "correct:  40018\n",
      "wrong:  9982\n",
      "accuracy: 80.036 %\n",
      "error: 19.964 %\n",
      "15469.817434\n",
      "correct:  40019\n",
      "wrong:  9981\n",
      "accuracy: 80.038 %\n",
      "error: 19.962 %\n",
      "15469.5472119\n",
      "correct:  40020\n",
      "wrong:  9980\n",
      "accuracy: 80.04 %\n",
      "error: 19.96 %\n",
      "15469.2768823\n",
      "correct:  40020\n",
      "wrong:  9980\n",
      "accuracy: 80.04 %\n",
      "error: 19.96 %\n",
      "15469.0067244\n",
      "correct:  40021\n",
      "wrong:  9979\n",
      "accuracy: 80.042 %\n",
      "error: 19.958 %\n",
      "15468.7368301\n",
      "correct:  40022\n",
      "wrong:  9978\n",
      "accuracy: 80.044 %\n",
      "error: 19.956 %\n",
      "15468.4669083\n",
      "correct:  40022\n",
      "wrong:  9978\n",
      "accuracy: 80.044 %\n",
      "error: 19.956 %\n",
      "15468.1971367\n",
      "correct:  40023\n",
      "wrong:  9977\n",
      "accuracy: 80.046 %\n",
      "error: 19.954 %\n",
      "15467.9272524\n",
      "20 iterations took 2264.690640449524 seconds.\n",
      "correct:  8212\n",
      "wrong:  1788\n",
      "accuracy: 82.12 %\n",
      "error: 17.88 %\n",
      "correct:  8120\n",
      "wrong:  1880\n",
      "accuracy: 81.2 %\n",
      "error: 18.8 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8120, 10000)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1\n",
    "b.validation(valid_set)\n",
    "b.validation(test_set)\n",
    "train2 = b.gradientDescent(train_set, 100, 0.1)\n",
    "b.validation(valid_set)\n",
    "b.validation(test_set)\n",
    "train3 = b.gradientDescent(train_set, 20, 0.01)\n",
    "b.validation(valid_set)\n",
    "b.validation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
