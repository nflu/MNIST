{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ..., 8 4 8]\n"
     ]
    }
   ],
   "source": [
    "import _pickle as cPickle\n",
    "import gzip \n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f,encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "print(train_set[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#these are just various mathematical tools needed for the NN\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "    return expit(x)*(1-expit(x)) #use expit to prevent overflow with large values\n",
    "\n",
    "def sigmoid(x):\n",
    "    return expit(x) #admittedly this is a little unnecessary but I think it makes sense to have \n",
    "    #sigmoid and sigmoidPrime instead of expit and sigmoidPrime\n",
    "\n",
    "def relu(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def reluPrime(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def elementWise(f, x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = f(x[i])\n",
    "    return x    \n",
    "\n",
    "def softMax(x):\n",
    "    z = np.exp(x)\n",
    "    return z/sum(z)\n",
    "\n",
    "def softMaxPrime(x):\n",
    "    z = np.exp(x)\n",
    "    c = sum(z)\n",
    "    for i in range(len(z)):\n",
    "        z[i] = (c-z[i])*z[i]/(c**2)\n",
    "    return z\n",
    "\n",
    "def display(x, label, act):\n",
    "    strn = \"\"\n",
    "    for i in range(len(x)):\n",
    "        if i%28==0:\n",
    "            print(strn)\n",
    "            strn = \"\"\n",
    "        if x[i]==0:\n",
    "            strn += \" \"\n",
    "        if x[i]>=act:\n",
    "            strn += \"x\"\n",
    "    print(label)\n",
    "            \n",
    "def logLoss(x, target):\n",
    "    loss = 0\n",
    "    for i in range(len(x)):\n",
    "        loss += target[i]*np.log(x[i])+(1-target[i])*np.log(1-x[i])\n",
    "    return (-1.0/len(x))*loss\n",
    "    \n",
    "def logLossPrime(x, target):\n",
    "    grad = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        grad[i]=(-1.0/len(x))*(target[i]/x[i]-(1-target[i])/(1-x[i]))\n",
    "    return grad\n",
    "\n",
    "def MSE(x, target):\n",
    "    a = x-target\n",
    "    return np.dot(a,a)\n",
    "\n",
    "def MSEPrime(x, target):\n",
    "    return 2*(x-target)\n",
    "\n",
    "def crossEntropy(output, target):\n",
    "    cost = 0\n",
    "    for i in range(len(target)):\n",
    "        cost -= target[i]*np.log(output[i])\n",
    "    return cost\n",
    "\n",
    "def trainingGraph(trainingData):\n",
    "    costs = trainingData[0]\n",
    "    accuracies = trainingData[1]\n",
    "    plt.plot(costs, 'ro')\n",
    "    plt.ylabel(\"total cost\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.show()\n",
    "    plt.plot(100.0*(1.0-accuracies), 'bo')\n",
    "    plt.plot(\"total error percentage\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "class Neural_Network:\n",
    "    defaultSize = 16 #default number of neurons in hidden layers if no shape list given\n",
    "    inputChecks = True #this will change whether inputs that match the MNIST format are given\n",
    "    #can be turned off to allow for debugging on smaller examples\n",
    "    \n",
    "    #activation must be an activation function that works for a single scalar \n",
    "    #and activation prime is its derivative. costFunction is a cost function for a vector representing an output layer\n",
    "    #and the target vector. costDeriv is its derivative with respect to the vector of activations.\n",
    "    #shape is list of integers where shape[i] = the number of neurons in ith layer\n",
    "    #layers is an integer describing the number of layers\n",
    "    #shape and layers are optional. if both are given the shape list will be followed, if neither are given\n",
    "    #it will use the default value for layers and make all hidden layers have defaultSize many neurons\n",
    "    def __init__(self, activation, activationPrime, costFunction, costDeriv, shape = None, layers = 4):\n",
    "        if layers<2:\n",
    "            raise NameError(\"Too few layers. Need an input layer and an output layer.\")\n",
    "        if Neural_Network.inputChecks and (shape[0] != 28**2 or shape[len(shape)-1] != 10):\n",
    "            raise NameError(\"Improper input or output layer size. \\\n",
    "                            Must be 28^2 input neurons and 10 output neurons to work with MNIST.\")\n",
    "        if shape==None:\n",
    "            shape = [28**2] + [defaultSize for i in range(layers-2)] + [10]\n",
    "        self.activation = activation\n",
    "        self.activationPrime = activationPrime\n",
    "        self.shape = shape\n",
    "        self.costFunction = costFunction\n",
    "        self.costDeriv = costDeriv\n",
    "        self.weights = Neural_Network.constructWeights(shape) \n",
    "        #weights[i] is the weights going into layer i\n",
    "        self.bias = Neural_Network.constructBias(shape)\n",
    "        #bias[i] is the bias on layer i \n",
    "        self.activations = []\n",
    "        self.zs = []\n",
    "        \n",
    "    #returns a list of matrices of weights. weights[i] is the set of weights going into ith layer\n",
    "    #weights[i][j][k] represents the weight going from the kth neuron in layer i-1 to jth neuron in layer i \n",
    "    def constructWeights(shape):\n",
    "        weights = [None]\n",
    "        for i in range(len(shape)-1): \n",
    "            weights.append(np.random.uniform(-1,1,shape[i]*shape[i+1]).reshape((shape[i+1],shape[i])))\n",
    "        return weights\n",
    "    \n",
    "    #returns a list of vectors of biases. bias[i] is the set of biases on the ith layer\n",
    "    #bias[i][j] is the bias in the ith layer on the jth neuron\n",
    "    def constructBias(shape):\n",
    "        bias = [None]\n",
    "        for i in range(1,len(shape)): \n",
    "            bias.append(np.random.uniform(-1,1,shape[i]))\n",
    "        return bias\n",
    "    \n",
    "    #performs forward propagation on the input x with the current weights and biases\n",
    "    #using activation function from the constructor returns the activations on the last layer \n",
    "    #updates the zs and activations attributes\n",
    "    def forwardProp(self, x):\n",
    "        if Neural_Network.inputChecks and len(x) != 28**2:\n",
    "            raise NameError(\"improper input\")\n",
    "        prevAct = x\n",
    "        act = []\n",
    "        self.activations = []\n",
    "        self.zs = []\n",
    "        self.activations.append(prevAct)\n",
    "        for i in range(1,len(self.shape)): #each layer\n",
    "            z = np.dot(self.weights[i], prevAct) + self.bias[i]\n",
    "            act = elementWise(self.activation, z)\n",
    "            self.activations.append(act)\n",
    "            self.zs.append(z)\n",
    "            prevAct = act\n",
    "        return act\n",
    "    \n",
    "    #returns the total cost for the neural network on all datapoints in data\n",
    "    #using cost function costFunc for each point. returns cost as a scalar\n",
    "    def totalCost(self, data, costFunc):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        if Neural_Network.inputChecks and len(images) != len(labels):\n",
    "            raise NameError(\"improper input\")\n",
    "        target = np.zeros(10)\n",
    "        cost = 0 \n",
    "        for i in range(len(images)):\n",
    "            target[labels[i]] = 1\n",
    "            out = self.forwardProp(images[i])\n",
    "            cost += costFunc(out, target)\n",
    "            target[labels[i]] = 0\n",
    "        return cost\n",
    "        \n",
    "    #returns the classification as a scalar based on the outputActivations\n",
    "    #picks the index with highest activation\n",
    "    def classification(outputActivations):\n",
    "        maximum = -1\n",
    "        index = -1\n",
    "        for i in range(len(outputActivations)):\n",
    "            if outputActivations[i] >= maximum:\n",
    "                index = i\n",
    "                maximum = outputActivations[i]\n",
    "        return index\n",
    "     \n",
    "    #randomly initializes all weights and biases    \n",
    "    def randomInitialization(self):\n",
    "        self.weights = Neural_Network.constructWeights(self.shape) \n",
    "        self.bias = Neural_Network.constructBias(self.shape)\n",
    "    \n",
    "    #using backProp this will perform gradient descent from the current initialization of the weights\n",
    "    #and biases on the given data. data in form touple of array of images and array of labels\n",
    "    #will stop after iterations many iterations. returns a touple of the best weights and biases\n",
    "    #and a list of the costs over time. will update the weights and biases in the neural network\n",
    "    def gradientDescent(self, data, iterations, learningRate):\n",
    "        return self.stochasticGradientDescent(data, len(data[0]), iterations, learningRate)\n",
    "    \n",
    "    #will apply the gradient with stepSize where the gradient is in form \n",
    "    #given by the backProp function\n",
    "    def applyGradient(self, gradient, stepSize):\n",
    "        for i in range(1, len(gradient[0])): #for each matrix in the weight update, \n",
    "            #first value is None for convenience in indexing\n",
    "            self.weights[i] -= stepSize * gradient[0][i]\n",
    "        for i in range(1, len(gradient[1])): #for each vector in the bias update\n",
    "            self.bias[i] -= stepSize * gradient[1][i]\n",
    "    \n",
    "    #computes the correct and incorrect number of classified data points on data\n",
    "    #prints the number correct, number incorrect, percent correct, and percent incorrect\n",
    "    #returns a touple of the number correct and total number of data points\n",
    "    def validation(self, data):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        if len(images) != len(labels):\n",
    "            raise NameError(\"improper input\")\n",
    "        correct = 0 \n",
    "        wrong = 0\n",
    "        for i in range(len(images)):\n",
    "            if (Neural_Network.classification(self.forwardProp(images[i]))) == labels[i]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        print(\"correct: \", correct)\n",
    "        print(\"wrong: \", wrong)\n",
    "        print(\"accuracy:\", 100.0*correct/len(images), \"%\")\n",
    "        print(\"error:\", 100.0*wrong/len(images), \"%\")\n",
    "        return (correct, len(images))\n",
    "    \n",
    "    #chooses a random batch of size batchSize from images and labels\n",
    "    #note it does not replace when sampling\n",
    "    def randomBatch(images, labels, batchSize):\n",
    "        indices = np.random.choice(len(images), batchSize, replace = False)\n",
    "        #if you set replace to True this will break the gradientDescent function\n",
    "        imageBatch = []\n",
    "        labelBatch = []\n",
    "        for i in range(len(indices)):\n",
    "            imageBatch.append(images[indices[i]])\n",
    "            labelBatch.append(labels[indices[i]])\n",
    "        return (imageBatch, labelBatch)\n",
    "        \n",
    "    #using backProp this will perform stoachastic gradient descent from the current initialization of the weights\n",
    "    #and biases on the given data. will choose batchSize many samples from data \n",
    "    #data in form touple of array of images and array of labels\n",
    "    #will stop after iterations many iterations. returns a touple of the best weights and biases\n",
    "    #and a list of the costs over time. will update the weights and biases in the neural network\n",
    "    def stochasticGradientDescent(self, data, batchSize, iterations, learningRate):\n",
    "        start = time.time()\n",
    "        bestCost = sys.maxsize\n",
    "        bestWandB = []\n",
    "        costs = []\n",
    "        accuracies = []\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        for _ in range(iterations):\n",
    "            for i in range(5): #performs 5 gradient steps before re-computing the cost for all examples\n",
    "                #this is somewhat of a hyper-parameter that just allows things to run pretty fast\n",
    "                imageSet, labelSet = Neural_Network.randomBatch(images, labels, batchSize)\n",
    "                target = np.zeros(10)\n",
    "                gradients = []\n",
    "                for i in range(len(imageSet)):\n",
    "                    target[labelSet[i]] = 1\n",
    "                    output = self.forwardProp(imageSet[i])\n",
    "                    gradients.append(self.backProp(target))\n",
    "                    target[labelSet[i]] = 0\n",
    "                self.applyGradient(self.averageGradient(gradients), learningRate)\n",
    "            cost = self.totalCost(data, self.costFunction)\n",
    "            costs.append(cost)\n",
    "            a = self.validation(data)\n",
    "            accuracies.append(a[0]/a[1])\n",
    "            print(cost)\n",
    "            if cost <= bestCost:\n",
    "                bestCost = cost\n",
    "                bestWandB = (self.weights, self.bias)\n",
    "        trainingData = (costs, accuracies)\n",
    "        end = time.time()\n",
    "        print(iterations, \"iterations took\", end - start, \"seconds.\")\n",
    "        return (bestWandB, trainingData)\n",
    "    \n",
    "    #given a list of touples of the gradient in the form given by backProp\n",
    "    #will return the average gradient\n",
    "    def averageGradient(self, gradients):\n",
    "        averageGrad = (Neural_Network.constructWeights(self.shape), Neural_Network.constructBias(self.shape))\n",
    "        for i in range(len(averageGrad)): #weights then biases\n",
    "            for j in range(len(gradients)): #grad from each sample\n",
    "                for k in range(1, len(gradients[j][i])): #grad for each matrix\n",
    "                    averageGrad[i][k] += gradients[j][i][k]\n",
    "            for k in range(1,len(averageGrad[i])):#done at end to prevent rounding small numbers to zero\n",
    "                averageGrad[i][k] /= len(gradients) #if overflow is a problem then divide at each step\n",
    "        return averageGrad\n",
    "    \n",
    "    #this will return the gradient of the cost on the single target\n",
    "    #the form is a touple with the weights and then the biases\n",
    "    #in the same format as the weights and biases attributes for the Neural_Network class\n",
    "    def backProp(self, target):\n",
    "        if Neural_Network.inputChecks and len(target) != 10:\n",
    "            raise NameError(\"improper input\")\n",
    "        biasGrad = [None for _ in range(len(self.shape))]\n",
    "        weightGrad = [None for _ in range(len(self.shape))]\n",
    "        delta = self.costDeriv(self.activations[-1], target) * self.activationPrime(self.zs[-1])\n",
    "        biasGrad[-1] = delta\n",
    "        weightGrad[-1] = np.tile(np.array([delta]).transpose(), (1,self.shape[-2]))*np.tile(np.array(self.activations[-2]),(self.shape[-1],1))\n",
    "        #this line (and the version of it below can be a little confusing, see readme)\n",
    "        for l in range(2, len(self.shape)):\n",
    "            z = self.zs[-l]\n",
    "            actDeriv = self.activationPrime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * actDeriv\n",
    "            biasGrad[-l] = delta\n",
    "            weightGrad[-l] = np.tile(np.array([delta]).transpose(), (1,self.shape[-l-1]))*np.tile(np.array(self.activations[-l-1]),(self.shape[-l],1))\n",
    "        return (weightGrad, biasGrad)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line right before the for loop in ```backProp()``` is a little confusing so I'll explain. $\\frac{\\partial C}{\\partial W^{l}_{j,k}} = a^{l-1}_{k} \\Delta^{l}_{j}$ where $C$ is the cost and $W^{l}_{j,k}$ is the weight going from the $k$th neuron in the $l-1$th layer to the $j$th neuron in the $l$th layer, $a^{l-1}_{k}$ is the activation on the $k$th neuron in the $l-1$th layer, and  $\\Delta^{l}_{j}$ is the derivative of the cost with respect to the $j$th component of $z^{l}$. If you've never seen this before, it can be confusing so draw out a simple NN with very few neurons and write out the gradient of the weight matrix. You will notice that if you make a matrix with the same dimensions as $W^{l}$ where the columns are the vector $\\Delta^{l}$ and the same dimension matrix where the rows are the row vector $(a^{l-1})^{T}$ and do element-wise multiplication (not matrix multiplication) of these matrices, then the resulting matrix is the gradient of the cost with respect to the weight matrix $W^{l}$.\n",
    "\n",
    "```np.tile``` allows you to create matrices from repeating column or row vectors. The [NumPy documentation](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.tile.html) explains it much better than I can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  11104\n",
      "wrong:  38896\n",
      "accuracy: 22.208 %\n",
      "error: 77.792 %\n",
      "44789.5725365\n",
      "correct:  14005\n",
      "wrong:  35995\n",
      "accuracy: 28.01 %\n",
      "error: 71.99 %\n",
      "43584.0692002\n",
      "correct:  22897\n",
      "wrong:  27103\n",
      "accuracy: 45.794 %\n",
      "error: 54.206 %\n",
      "40019.6974792\n",
      "correct:  23590\n",
      "wrong:  26410\n",
      "accuracy: 47.18 %\n",
      "error: 52.82 %\n",
      "38143.7754353\n",
      "correct:  29943\n",
      "wrong:  20057\n",
      "accuracy: 59.886 %\n",
      "error: 40.114 %\n",
      "33246.2197922\n",
      "correct:  30307\n",
      "wrong:  19693\n",
      "accuracy: 60.614 %\n",
      "error: 39.386 %\n",
      "31592.0676809\n",
      "correct:  33231\n",
      "wrong:  16769\n",
      "accuracy: 66.462 %\n",
      "error: 33.538 %\n",
      "28148.3615751\n",
      "correct:  34328\n",
      "wrong:  15672\n",
      "accuracy: 68.656 %\n",
      "error: 31.344 %\n",
      "27126.4580038\n",
      "correct:  34458\n",
      "wrong:  15542\n",
      "accuracy: 68.916 %\n",
      "error: 31.084 %\n",
      "25714.3637737\n"
     ]
    }
   ],
   "source": [
    "b = Neural_Network(expit, sigmoidPrime, MSE, MSEPrime, shape=[28**2, 30, 30, 10])\n",
    "train1 = b.stochasticGradientDescent(train_set, 2000, 40, 5)\n",
    "b.validate(valid_set)\n",
    "b.validate(test_set)\n",
    "train2 = b.gradientDescent(train_set, 100, 0.1)\n",
    "b.validate(valid_set)\n",
    "b.validate(test_set)\n",
    "train3 = b.gradientDescent(train_set, 20, 0.01)\n",
    "b.validate(valid_set)\n",
    "b.validate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
