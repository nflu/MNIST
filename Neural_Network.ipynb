{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 8 4 8]\n"
     ]
    }
   ],
   "source": [
    "import _pickle as cPickle\n",
    "import pickle\n",
    "import gzip \n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f,encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "print(train_set[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#these are just various mathematical tools needed for the NN\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def identityPrime(x):\n",
    "    return np.ones(len(x))\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "    return expit(x)*(1-expit(x)) #use expit to prevent overflow with large values\n",
    "\n",
    "def sigmoid(x):\n",
    "    return expit(x) #admittedly this is a little unnecessary but I think it makes sense to have \n",
    "    #sigmoid and sigmoidPrime instead of expit and sigmoidPrime\n",
    "\n",
    "def relu(x):\n",
    "    if x<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def reluPrime(x):\n",
    "    z = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        if x[i]>0:\n",
    "            z[i] = 1\n",
    "    return z\n",
    "\n",
    "def elementWise(f, x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = f(x[i])\n",
    "    return x    \n",
    "\n",
    "def softMax(x):\n",
    "    z = np.exp(x)\n",
    "    return z/sum(z)\n",
    "\n",
    "def softMaxPrime(x):\n",
    "    z = np.exp(x)\n",
    "    c = sum(z)\n",
    "    for i in range(len(z)):\n",
    "        z[i] = (c-z[i])*z[i]/(c*c)\n",
    "    return z\n",
    "\n",
    "def logLoss(x, target):\n",
    "    loss = 0\n",
    "    for i in range(len(x)):\n",
    "        loss += target[i]*np.log(x[i])+(1-target[i])*np.log(1-x[i])\n",
    "    return (-1.0/len(x))*loss\n",
    "    \n",
    "def logLossPrime(x, target):\n",
    "    grad = np.zeros(len(x))\n",
    "    for i in range(len(x)):\n",
    "        grad[i]=(-1.0/len(x))*(target[i]/x[i]-(1-target[i])/(1-x[i]))\n",
    "    return grad\n",
    "\n",
    "def MSE(x, target):\n",
    "    a = x-target\n",
    "    return np.dot(a,a)\n",
    "\n",
    "def MSEPrime(x, target):\n",
    "    return 2*(x-target)\n",
    "\n",
    "def crossEntropy(output, target):\n",
    "    cost = 0\n",
    "    for i in range(len(target)):\n",
    "        cost -= target[i]*np.log(output[i])\n",
    "    return cost\n",
    "\n",
    "def crossEntropyPrime(output, target):\n",
    "    return -target/output\n",
    "\n",
    "def softMaxCross(x, target):\n",
    "    return crossEntropy(softMax(x), target)\n",
    "\n",
    "def softMaxCrossPrime(x, target):\n",
    "    return softMax(x)-target\n",
    "\n",
    "def trainingGraph(trainingData):\n",
    "    costs = trainingData[0]\n",
    "    accuracies = trainingData[1]\n",
    "    plt.plot(costs, 'ro')\n",
    "    plt.ylabel(\"total cost\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.show()\n",
    "    plt.plot(100.0*(1.0-accuracies), 'bo')\n",
    "    plt.plot(\"total error percentage\")\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.show()\n",
    "\n",
    "class Neural_Network:\n",
    "    defaultSize = 16 #default number of neurons in hidden layers if no shape list given\n",
    "    inputChecks = True #this will change whether inputs that match the MNIST format are given\n",
    "    #can be turned off to allow for debugging on smaller examples\n",
    "    \n",
    "    #activation must be an activation function that works for a single scalar \n",
    "    #and activation prime is its derivative. costFunction is a cost function for a vector representing an output layer\n",
    "    #and the target vector. costDeriv is its derivative with respect to the vector of activations.\n",
    "    #shape is list of integers where shape[i] = the number of neurons in ith layer\n",
    "    #layers is an integer describing the number of layers, lastAct is an optional change so that on the last layer\n",
    "    #you can have a different activation function\n",
    "    #shape and layers are optional. if both are given the shape list will be followed, if neither are given\n",
    "    #it will use the default value for layers and make all hidden layers have defaultSize many neurons\n",
    "    #lastAct and lastActPrime are the activation function and its derivative for the last layer. if left blank\n",
    "    #will default to the given activation and activationPrime\n",
    "    def __init__(self, activation, activationPrime, costFunction, costDeriv, shape = None, layers = 4, \n",
    "                 lastAct = None, lastActPrime = None):\n",
    "        if layers<2:\n",
    "            raise NameError(\"Too few layers. Need an input layer and an output layer.\")\n",
    "        if Neural_Network.inputChecks and (shape[0] != 28**2 or shape[len(shape)-1] != 10):\n",
    "            raise NameError(\"Improper input or output layer size. \\\n",
    "                            Must be 28^2 input neurons and 10 output neurons to work with MNIST.\")\n",
    "        if shape==None:\n",
    "            shape = [28**2] + [defaultSize for i in range(layers-2)] + [10]\n",
    "        self.activation = activation\n",
    "        self.activationPrime = activationPrime\n",
    "        self.shape = shape\n",
    "        self.costFunction = costFunction\n",
    "        self.costDeriv = costDeriv\n",
    "        self.weights = Neural_Network.constructWeights(shape) \n",
    "        #weights[i] is the weights going into layer i\n",
    "        self.bias = Neural_Network.constructBias(shape)\n",
    "        #bias[i] is the bias on layer i\n",
    "        if lastAct == None:\n",
    "            self.lastAct = activation\n",
    "            self.lastActPrime = activationPrime\n",
    "        else:\n",
    "            self.lastAct = lastAct\n",
    "            self.lastActPrime = lastActPrime\n",
    "        self.activations = []\n",
    "        self.zs = []\n",
    "        \n",
    "    #returns a list of matrices of weights. weights[i] is the set of weights going into ith layer\n",
    "    #weights[i][j][k] represents the weight going from the kth neuron in layer i-1 to jth neuron in layer i \n",
    "    def constructWeights(shape):\n",
    "        weights = [None]\n",
    "        for i in range(len(shape)-1): \n",
    "            weights.append(np.random.uniform(-1,1,shape[i]*shape[i+1]).reshape((shape[i+1],shape[i])))\n",
    "        return weights\n",
    "    \n",
    "    #returns a list of vectors of biases. bias[i] is the set of biases on the ith layer\n",
    "    #bias[i][j] is the bias in the ith layer on the jth neuron\n",
    "    def constructBias(shape):\n",
    "        bias = [None]\n",
    "        for i in range(1,len(shape)): \n",
    "            bias.append(np.random.uniform(-1,1,shape[i]))\n",
    "        return bias\n",
    "    \n",
    "    #performs forward propagation on the input x with the current weights and biases\n",
    "    #using activation function from the constructor returns the activations on the last layer \n",
    "    #updates the zs and activations attributes\n",
    "    def forwardProp(self, x):\n",
    "        if Neural_Network.inputChecks and len(x) != 28**2:\n",
    "            raise NameError(\"improper input\")\n",
    "        prevAct = x\n",
    "        act = []\n",
    "        self.activations = []\n",
    "        self.zs = []\n",
    "        self.activations.append(prevAct)\n",
    "        for i in range(1,len(self.shape)-1): #each layer except for last\n",
    "            z = np.dot(self.weights[i], prevAct) + self.bias[i]\n",
    "            act = elementWise(self.activation, z)\n",
    "            self.activations.append(act)\n",
    "            self.zs.append(z)\n",
    "            prevAct = act\n",
    "        i = len(self.shape)-1    \n",
    "        z = np.dot(self.weights[i], prevAct) + self.bias[i]\n",
    "        act = elementWise(self.lastAct, z)\n",
    "        self.activations.append(act)\n",
    "        self.zs.append(z)\n",
    "        prevAct = act\n",
    "        return act\n",
    "    \n",
    "    #returns the total cost for the neural network on all datapoints in data\n",
    "    #using cost function costFunc for each point. returns cost as a scalar\n",
    "    def totalCost(self, data, costFunc):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        if Neural_Network.inputChecks and len(images) != len(labels):\n",
    "            raise NameError(\"improper input\")\n",
    "        target = np.zeros(10)\n",
    "        cost = 0 \n",
    "        for i in range(len(images)):\n",
    "            target[labels[i]] = 1\n",
    "            out = self.forwardProp(images[i])\n",
    "            cost += costFunc(out, target)\n",
    "            target[labels[i]] = 0\n",
    "        return cost\n",
    "        \n",
    "    #returns the classification as a scalar based on the outputActivations\n",
    "    #picks the index with highest activation\n",
    "    def classification(outputActivations):\n",
    "        maximum = -1\n",
    "        index = -1\n",
    "        for i in range(len(outputActivations)):\n",
    "            if outputActivations[i] >= maximum:\n",
    "                index = i\n",
    "                maximum = outputActivations[i]\n",
    "        return index\n",
    "     \n",
    "    #randomly initializes all weights and biases    \n",
    "    def randomInitialization(self):\n",
    "        self.weights = Neural_Network.constructWeights(self.shape) \n",
    "        self.bias = Neural_Network.constructBias(self.shape)\n",
    "    \n",
    "    #using backProp this will perform gradient descent from the current initialization of the weights\n",
    "    #and biases on the given data. data in form touple of array of images and array of labels\n",
    "    #will stop after iterations many iterations. returns a touple of the best weights and biases\n",
    "    #and a list of the costs over time. will update the weights and biases in the neural network\n",
    "    #suppressPrint will not print anything between steps if set to true\n",
    "    #costInterval determines how often the cost will be re-computed, saved, printed (if suppressPrint is false)\n",
    "    #and how often the best weights and biases are saved. when set to 1 it occurs on every step\n",
    "    def gradientDescent(self, data, iterations, learningRate, suppressPrint = False, costInterval = 1):\n",
    "        return self.stochasticGradientDescent(data, len(data[0]), iterations, learningRate, suppressPrint, costInterval)\n",
    "    \n",
    "    #will apply the gradient with stepSize where the gradient is in form \n",
    "    #given by the backProp function\n",
    "    def applyGradient(self, gradient, stepSize):\n",
    "        for i in range(1, len(gradient[0])): #for each matrix in the weight update, \n",
    "            #first value is None for convenience in indexing\n",
    "            self.weights[i] -= stepSize * gradient[0][i]\n",
    "        for i in range(1, len(gradient[1])): #for each vector in the bias update\n",
    "            self.bias[i] -= stepSize * gradient[1][i]\n",
    "    \n",
    "    #computes the correct and incorrect number of classified data points on data\n",
    "    #prints the number correct, number incorrect, percent correct, and percent incorrect\n",
    "    #returns a touple of the number correct and total number of data points\n",
    "    def validation(self, data, suppressPrint = False):\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        if len(images) != len(labels):\n",
    "            raise NameError(\"improper input\")\n",
    "        correct = 0 \n",
    "        wrong = 0\n",
    "        for i in range(len(images)):\n",
    "            if (Neural_Network.classification(self.forwardProp(images[i]))) == labels[i]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        if not suppressPrint:\n",
    "            print(\"correct: \", correct)\n",
    "            print(\"wrong: \", wrong)\n",
    "            print(\"accuracy:\", 100.0*correct/len(images), \"%\")\n",
    "            print(\"error:\", 100.0*wrong/len(images), \"%\")\n",
    "        return (correct, len(images))\n",
    "    \n",
    "    #chooses a random batch of size batchSize from images and labels\n",
    "    #note it does not replace when sampling\n",
    "    def randomBatch(images, labels, batchSize):\n",
    "        indices = np.random.choice(len(images), batchSize, replace = False)\n",
    "        #if you set replace to True this will break the gradientDescent function\n",
    "        imageBatch = []\n",
    "        labelBatch = []\n",
    "        for i in range(len(indices)):\n",
    "            imageBatch.append(images[indices[i]])\n",
    "            labelBatch.append(labels[indices[i]])\n",
    "        return (imageBatch, labelBatch)\n",
    "        \n",
    "    #using backProp this will perform stoachastic gradient descent from the current initialization of the weights\n",
    "    #and biases on the given data. will choose batchSize many samples from data \n",
    "    #data in form touple of array of images and array of labels\n",
    "    #will stop after iterations many iterations. returns a touple of the best weights and biases\n",
    "    #and a list of the costs over time. will update the weights and biases in the neural network\n",
    "    #suppressPrint will not print anything between steps if set to true\n",
    "    #costInterval determines how often the cost will be re-computed, saved, printed (if suppressPrint is false)\n",
    "    #and how often the best weights and biases are saved. when set to 1 it occurs on every step\n",
    "    def stochasticGradientDescent(self, data, batchSize, iterations, learningRate, suppressPrint = False, costInterval = 5):\n",
    "        start = time.time()\n",
    "        bestCost = sys.maxsize\n",
    "        bestWandB = []\n",
    "        costs = []\n",
    "        accuracies = []\n",
    "        images = data[0]\n",
    "        labels = data[1]\n",
    "        for j in range(iterations):\n",
    "            imageSet, labelSet = Neural_Network.randomBatch(images, labels, batchSize)\n",
    "            target = np.zeros(10)\n",
    "            gradients = []\n",
    "            for i in range(len(imageSet)):\n",
    "                target[labelSet[i]] = 1\n",
    "                output = self.forwardProp(imageSet[i])\n",
    "                gradients.append(self.backProp(target))\n",
    "                target[labelSet[i]] = 0\n",
    "            self.applyGradient(self.averageGradient(gradients), learningRate)\n",
    "            if j % costInterval == 0: #performs this many gradient steps before re-computing the cost for all examples\n",
    "                #this is somewhat of a hyper-parameter. Higher number = faster runtime but less training data, \n",
    "                #less info when it is running about how it is doing, and fewer opportunities to save the best configuration\n",
    "                cost = self.totalCost(data, self.costFunction)\n",
    "                costs.append(cost)\n",
    "                a = self.validation(data, suppressPrint)\n",
    "                accuracies.append(a[0]/a[1])\n",
    "                if not suppressPrint:\n",
    "                    print(cost)\n",
    "                if cost <= bestCost:#this saves the best configuration so far but only on steps when the cost is computed\n",
    "                    bestCost = cost\n",
    "                    bestWandB = (self.weights, self.bias)\n",
    "        trainingData = (costs, accuracies)\n",
    "        end = time.time()\n",
    "        print(iterations, \"iterations took\", end - start, \"seconds.\")\n",
    "        return (bestWandB, trainingData)\n",
    "    \n",
    "    #given a list of touples of the gradient in the form given by backProp\n",
    "    #will return the average gradient\n",
    "    def averageGradient(self, gradients):\n",
    "        averageGrad = (Neural_Network.constructWeights(self.shape), Neural_Network.constructBias(self.shape))\n",
    "        for i in range(len(averageGrad)): #weights then biases\n",
    "            for j in range(len(gradients)): #grad from each sample\n",
    "                for k in range(1, len(gradients[j][i])): #grad for each matrix\n",
    "                    averageGrad[i][k] += gradients[j][i][k]\n",
    "            for k in range(1,len(averageGrad[i])):#done at end to prevent rounding small numbers to zero\n",
    "                averageGrad[i][k] /= len(gradients) #if overflow is a problem then divide at each step\n",
    "        return averageGrad\n",
    "    \n",
    "    #this will return the gradient of the cost on the single target\n",
    "    #the form is a touple with the weights and then the biases\n",
    "    #in the same format as the weights and biases attributes for the Neural_Network class\n",
    "    def backProp(self, target):\n",
    "        if Neural_Network.inputChecks and len(target) != 10:\n",
    "            raise NameError(\"improper input\")\n",
    "        biasGrad = [None for _ in range(len(self.shape))]\n",
    "        weightGrad = [None for _ in range(len(self.shape))]\n",
    "        delta = self.costDeriv(self.activations[-1], target) * self.lastActPrime(self.zs[-1])\n",
    "        biasGrad[-1] = delta\n",
    "        weightGrad[-1] = np.tile(np.array([delta]).transpose(), (1,self.shape[-2]))*np.tile(np.array(self.activations[-2]),(self.shape[-1],1))\n",
    "        #this line (and the version of it below can be a little confusing, see readme)\n",
    "        for l in range(2, len(self.shape)):\n",
    "            z = self.zs[-l]\n",
    "            actDeriv = self.activationPrime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * actDeriv\n",
    "            biasGrad[-l] = delta\n",
    "            weightGrad[-l] = np.tile(np.array([delta]).transpose(), (1,self.shape[-l-1]))*np.tile(np.array(self.activations[-l-1]),(self.shape[-l],1))\n",
    "        return (weightGrad, biasGrad)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line right before the for loop in ```backProp()``` is a little confusing so I'll explain. $\\frac{\\partial C}{\\partial W^{l}_{j,k}} = a^{l-1}_{k} \\Delta^{l}_{j}$ where $C$ is the cost and $W^{l}_{j,k}$ is the weight going from the $k$th neuron in the $l-1$th layer to the $j$th neuron in the $l$th layer, $a^{l-1}_{k}$ is the activation on the $k$th neuron in the $l-1$th layer, and $\\Delta^{l}_{j}$ is the derivative of the cost with respect to the $j$th component of $z^{l}$. \n",
    "\n",
    "If you've never seen this before, it can be confusing so draw out a simple NN with very few neurons and write out the gradient of the weight matrix. You will notice that if you make a matrix with the same dimensions as $W^{l}$ where the columns are the vector $\\Delta^{l}$ and the same dimension matrix where the rows are the row vector $(a^{l-1})^{T}$ and do element-wise multiplication (not matrix multiplication) of these matrices, then the resulting matrix is the gradient of the cost with respect to the weight matrix $W^{l}$.\n",
    "\n",
    "If you're more comfortable with index notation you can also just see that in the gradient matrix of the cost with respect to the weight $W^{l}$ weights with the same row have the same $\\Delta$ value being multiplied. Similarly weights with the same column have the same $a^{l-1}$ value being multiplied.\n",
    "\n",
    "```np.tile``` allows you to create matrices from repeating column or row vectors. The [NumPy documentation](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.tile.html) explains it much better than I can. Thus in this line I use ```np.tile``` to create a matrix where the columns are the vector $\\Delta^{l}$ and a matrix where the rows are $(a^{l-1})^{T}$ and then perform element-wise multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  13522\n",
      "wrong:  36478\n",
      "accuracy: 27.044 %\n",
      "error: 72.956 %\n",
      "176973.66519770512\n",
      "correct:  21712\n",
      "wrong:  28288\n",
      "accuracy: 43.424 %\n",
      "error: 56.576 %\n",
      "122368.66139624966\n",
      "correct:  26222\n",
      "wrong:  23778\n",
      "accuracy: 52.444 %\n",
      "error: 47.556 %\n",
      "96210.09302097904\n",
      "correct:  29227\n",
      "wrong:  20773\n",
      "accuracy: 58.454 %\n",
      "error: 41.546 %\n",
      "80893.85712997772\n",
      "correct:  31344\n",
      "wrong:  18656\n",
      "accuracy: 62.688 %\n",
      "error: 37.312 %\n",
      "70947.89240534272\n",
      "correct:  32967\n",
      "wrong:  17033\n",
      "accuracy: 65.934 %\n",
      "error: 34.066 %\n",
      "63995.13015699213\n",
      "correct:  34212\n",
      "wrong:  15788\n",
      "accuracy: 68.424 %\n",
      "error: 31.576 %\n",
      "58849.79254094217\n",
      "correct:  35209\n",
      "wrong:  14791\n",
      "accuracy: 70.418 %\n",
      "error: 29.582 %\n",
      "54871.116992258336\n",
      "correct:  36048\n",
      "wrong:  13952\n",
      "accuracy: 72.096 %\n",
      "error: 27.904 %\n",
      "51688.964925836815\n",
      "correct:  36701\n",
      "wrong:  13299\n",
      "accuracy: 73.402 %\n",
      "error: 26.598 %\n",
      "49075.84210750774\n",
      "correct:  37248\n",
      "wrong:  12752\n",
      "accuracy: 74.496 %\n",
      "error: 25.504 %\n",
      "46885.24992997189\n",
      "correct:  37745\n",
      "wrong:  12255\n",
      "accuracy: 75.49 %\n",
      "error: 24.51 %\n",
      "45017.900109317095\n",
      "correct:  38202\n",
      "wrong:  11798\n",
      "accuracy: 76.404 %\n",
      "error: 23.596 %\n",
      "43403.71548763703\n",
      "correct:  38555\n",
      "wrong:  11445\n",
      "accuracy: 77.11 %\n",
      "error: 22.89 %\n",
      "41992.25667716676\n",
      "correct:  38874\n",
      "wrong:  11126\n",
      "accuracy: 77.748 %\n",
      "error: 22.252 %\n",
      "40745.6364934652\n",
      "correct:  39182\n",
      "wrong:  10818\n",
      "accuracy: 78.364 %\n",
      "error: 21.636 %\n",
      "39634.84250325803\n",
      "correct:  39452\n",
      "wrong:  10548\n",
      "accuracy: 78.904 %\n",
      "error: 21.096 %\n",
      "38637.60365034283\n",
      "correct:  39683\n",
      "wrong:  10317\n",
      "accuracy: 79.366 %\n",
      "error: 20.634 %\n",
      "37735.99612993693\n",
      "correct:  39927\n",
      "wrong:  10073\n",
      "accuracy: 79.854 %\n",
      "error: 20.146 %\n",
      "36915.966386993634\n",
      "correct:  40134\n",
      "wrong:  9866\n",
      "accuracy: 80.268 %\n",
      "error: 19.732 %\n",
      "36165.996039681086\n",
      "correct:  40310\n",
      "wrong:  9690\n",
      "accuracy: 80.62 %\n",
      "error: 19.38 %\n",
      "35476.84620026517\n",
      "correct:  40479\n",
      "wrong:  9521\n",
      "accuracy: 80.958 %\n",
      "error: 19.042 %\n",
      "34840.81940978856\n",
      "correct:  40624\n",
      "wrong:  9376\n",
      "accuracy: 81.248 %\n",
      "error: 18.752 %\n",
      "34251.50665351602\n",
      "correct:  40759\n",
      "wrong:  9241\n",
      "accuracy: 81.518 %\n",
      "error: 18.482 %\n",
      "33703.358272848025\n",
      "correct:  40917\n",
      "wrong:  9083\n",
      "accuracy: 81.834 %\n",
      "error: 18.166 %\n",
      "33191.91464640616\n",
      "correct:  41029\n",
      "wrong:  8971\n",
      "accuracy: 82.058 %\n",
      "error: 17.942 %\n",
      "32713.3344339855\n",
      "correct:  41151\n",
      "wrong:  8849\n",
      "accuracy: 82.302 %\n",
      "error: 17.698 %\n",
      "32264.13421399353\n",
      "correct:  41237\n",
      "wrong:  8763\n",
      "accuracy: 82.474 %\n",
      "error: 17.526 %\n",
      "31841.390910325445\n",
      "correct:  41351\n",
      "wrong:  8649\n",
      "accuracy: 82.702 %\n",
      "error: 17.298 %\n",
      "31442.675409121537\n",
      "correct:  41437\n",
      "wrong:  8563\n",
      "accuracy: 82.874 %\n",
      "error: 17.126 %\n",
      "31065.799479660258\n",
      "correct:  41552\n",
      "wrong:  8448\n",
      "accuracy: 83.104 %\n",
      "error: 16.896 %\n",
      "30708.794755899544\n",
      "correct:  41648\n",
      "wrong:  8352\n",
      "accuracy: 83.296 %\n",
      "error: 16.704 %\n",
      "30370.014351830116\n",
      "correct:  41728\n",
      "wrong:  8272\n",
      "accuracy: 83.456 %\n",
      "error: 16.544 %\n",
      "30047.879399470763\n",
      "correct:  41819\n",
      "wrong:  8181\n",
      "accuracy: 83.638 %\n",
      "error: 16.362 %\n",
      "29741.16601296312\n",
      "correct:  41890\n",
      "wrong:  8110\n",
      "accuracy: 83.78 %\n",
      "error: 16.22 %\n",
      "29448.510827724316\n",
      "correct:  41959\n",
      "wrong:  8041\n",
      "accuracy: 83.918 %\n",
      "error: 16.082 %\n",
      "29168.957452565162\n",
      "correct:  42028\n",
      "wrong:  7972\n",
      "accuracy: 84.056 %\n",
      "error: 15.944 %\n",
      "28901.53231212655\n",
      "correct:  42091\n",
      "wrong:  7909\n",
      "accuracy: 84.182 %\n",
      "error: 15.818 %\n",
      "28645.43435992689\n",
      "correct:  42144\n",
      "wrong:  7856\n",
      "accuracy: 84.288 %\n",
      "error: 15.712 %\n",
      "28399.791025384042\n",
      "correct:  42220\n",
      "wrong:  7780\n",
      "accuracy: 84.44 %\n",
      "error: 15.56 %\n",
      "28163.89745770969\n",
      "correct:  42288\n",
      "wrong:  7712\n",
      "accuracy: 84.576 %\n",
      "error: 15.424 %\n",
      "27937.134634817554\n",
      "correct:  42343\n",
      "wrong:  7657\n",
      "accuracy: 84.686 %\n",
      "error: 15.314 %\n",
      "27718.967048105205\n",
      "correct:  42389\n",
      "wrong:  7611\n",
      "accuracy: 84.778 %\n",
      "error: 15.222 %\n",
      "27508.82072227809\n",
      "correct:  42439\n",
      "wrong:  7561\n",
      "accuracy: 84.878 %\n",
      "error: 15.122 %\n",
      "27306.204218792307\n",
      "correct:  42496\n",
      "wrong:  7504\n",
      "accuracy: 84.992 %\n",
      "error: 15.008 %\n",
      "27110.696162020464\n",
      "correct:  42551\n",
      "wrong:  7449\n",
      "accuracy: 85.102 %\n",
      "error: 14.898 %\n",
      "26921.862376081328\n",
      "correct:  42600\n",
      "wrong:  7400\n",
      "accuracy: 85.2 %\n",
      "error: 14.8 %\n",
      "26739.336479811132\n",
      "correct:  42640\n",
      "wrong:  7360\n",
      "accuracy: 85.28 %\n",
      "error: 14.72 %\n",
      "26562.753670886636\n",
      "correct:  42700\n",
      "wrong:  7300\n",
      "accuracy: 85.4 %\n",
      "error: 14.6 %\n",
      "26391.79963379803\n",
      "correct:  42750\n",
      "wrong:  7250\n",
      "accuracy: 85.5 %\n",
      "error: 14.5 %\n",
      "26226.15606024954\n",
      "correct:  42798\n",
      "wrong:  7202\n",
      "accuracy: 85.596 %\n",
      "error: 14.404 %\n",
      "26065.612526691173\n",
      "correct:  42840\n",
      "wrong:  7160\n",
      "accuracy: 85.68 %\n",
      "error: 14.32 %\n",
      "25909.870429559385\n",
      "correct:  42881\n",
      "wrong:  7119\n",
      "accuracy: 85.762 %\n",
      "error: 14.238 %\n",
      "25758.72635705395\n",
      "correct:  42923\n",
      "wrong:  7077\n",
      "accuracy: 85.846 %\n",
      "error: 14.154 %\n",
      "25611.89729664844\n",
      "correct:  42956\n",
      "wrong:  7044\n",
      "accuracy: 85.912 %\n",
      "error: 14.088 %\n",
      "25469.206155895834\n",
      "correct:  42985\n",
      "wrong:  7015\n",
      "accuracy: 85.97 %\n",
      "error: 14.03 %\n",
      "25330.50884709619\n",
      "correct:  43027\n",
      "wrong:  6973\n",
      "accuracy: 86.054 %\n",
      "error: 13.946 %\n",
      "25195.54236557084\n",
      "correct:  43066\n",
      "wrong:  6934\n",
      "accuracy: 86.132 %\n",
      "error: 13.868 %\n",
      "25064.14803531062\n",
      "correct:  43108\n",
      "wrong:  6892\n",
      "accuracy: 86.216 %\n",
      "error: 13.784 %\n",
      "24936.176850095933\n",
      "correct:  43142\n",
      "wrong:  6858\n",
      "accuracy: 86.284 %\n",
      "error: 13.716 %\n",
      "24811.57465217781\n",
      "correct:  43178\n",
      "wrong:  6822\n",
      "accuracy: 86.356 %\n",
      "error: 13.644 %\n",
      "24690.088727051287\n",
      "correct:  43212\n",
      "wrong:  6788\n",
      "accuracy: 86.424 %\n",
      "error: 13.576 %\n",
      "24571.619315322478\n",
      "correct:  43244\n",
      "wrong:  6756\n",
      "accuracy: 86.488 %\n",
      "error: 13.512 %\n",
      "24456.069797262215\n",
      "correct:  43266\n",
      "wrong:  6734\n",
      "accuracy: 86.532 %\n",
      "error: 13.468 %\n",
      "24343.25723350797\n",
      "correct:  43295\n",
      "wrong:  6705\n",
      "accuracy: 86.59 %\n",
      "error: 13.41 %\n",
      "24233.126771258892\n",
      "correct:  43325\n",
      "wrong:  6675\n",
      "accuracy: 86.65 %\n",
      "error: 13.35 %\n",
      "24125.566114672252\n",
      "correct:  43353\n",
      "wrong:  6647\n",
      "accuracy: 86.706 %\n",
      "error: 13.294 %\n",
      "24020.45286407099\n",
      "correct:  43391\n",
      "wrong:  6609\n",
      "accuracy: 86.782 %\n",
      "error: 13.218 %\n",
      "23917.747385637525\n",
      "correct:  43428\n",
      "wrong:  6572\n",
      "accuracy: 86.856 %\n",
      "error: 13.144 %\n",
      "23817.33237598851\n",
      "correct:  43459\n",
      "wrong:  6541\n",
      "accuracy: 86.918 %\n",
      "error: 13.082 %\n",
      "23719.115321216348\n",
      "correct:  43480\n",
      "wrong:  6520\n",
      "accuracy: 86.96 %\n",
      "error: 13.04 %\n",
      "23623.016581279902\n",
      "correct:  43513\n",
      "wrong:  6487\n",
      "accuracy: 87.026 %\n",
      "error: 12.974 %\n",
      "23528.993389419382\n",
      "correct:  43536\n",
      "wrong:  6464\n",
      "accuracy: 87.072 %\n",
      "error: 12.928 %\n",
      "23436.96657415548\n",
      "correct:  43548\n",
      "wrong:  6452\n",
      "accuracy: 87.096 %\n",
      "error: 12.904 %\n",
      "23346.853371884295\n",
      "correct:  43568\n",
      "wrong:  6432\n",
      "accuracy: 87.136 %\n",
      "error: 12.864 %\n",
      "23258.549075415547\n",
      "correct:  43588\n",
      "wrong:  6412\n",
      "accuracy: 87.176 %\n",
      "error: 12.824 %\n",
      "23172.05571860284\n",
      "correct:  43603\n",
      "wrong:  6397\n",
      "accuracy: 87.206 %\n",
      "error: 12.794 %\n",
      "23087.26283222366\n",
      "correct:  43621\n",
      "wrong:  6379\n",
      "accuracy: 87.242 %\n",
      "error: 12.758 %\n",
      "23004.152985058194\n",
      "correct:  43639\n",
      "wrong:  6361\n",
      "accuracy: 87.278 %\n",
      "error: 12.722 %\n",
      "22922.664905625843\n",
      "correct:  43660\n",
      "wrong:  6340\n",
      "accuracy: 87.32 %\n",
      "error: 12.68 %\n",
      "22842.73832108946\n",
      "correct:  43679\n",
      "wrong:  6321\n",
      "accuracy: 87.358 %\n",
      "error: 12.642 %\n",
      "22764.315166030858\n",
      "correct:  43703\n",
      "wrong:  6297\n",
      "accuracy: 87.406 %\n",
      "error: 12.594 %\n",
      "22687.367230641423\n",
      "correct:  43728\n",
      "wrong:  6272\n",
      "accuracy: 87.456 %\n",
      "error: 12.544 %\n",
      "22611.831968222035\n",
      "correct:  43749\n",
      "wrong:  6251\n",
      "accuracy: 87.498 %\n",
      "error: 12.502 %\n",
      "22537.69950283335\n",
      "correct:  43777\n",
      "wrong:  6223\n",
      "accuracy: 87.554 %\n",
      "error: 12.446 %\n",
      "22464.914149005086\n",
      "correct:  43788\n",
      "wrong:  6212\n",
      "accuracy: 87.576 %\n",
      "error: 12.424 %\n",
      "22393.41634470718\n",
      "correct:  43813\n",
      "wrong:  6187\n",
      "accuracy: 87.626 %\n",
      "error: 12.374 %\n",
      "22323.221005755513\n",
      "correct:  43835\n",
      "wrong:  6165\n",
      "accuracy: 87.67 %\n",
      "error: 12.33 %\n",
      "22254.235406954875\n",
      "correct:  43850\n",
      "wrong:  6150\n",
      "accuracy: 87.7 %\n",
      "error: 12.3 %\n",
      "22186.44810979532\n",
      "correct:  43872\n",
      "wrong:  6128\n",
      "accuracy: 87.744 %\n",
      "error: 12.256 %\n",
      "22119.844267408054\n",
      "correct:  43886\n",
      "wrong:  6114\n",
      "accuracy: 87.772 %\n",
      "error: 12.228 %\n",
      "22054.34985241541\n",
      "correct:  43902\n",
      "wrong:  6098\n",
      "accuracy: 87.804 %\n",
      "error: 12.196 %\n",
      "21989.932257584445\n",
      "correct:  43928\n",
      "wrong:  6072\n",
      "accuracy: 87.856 %\n",
      "error: 12.144 %\n",
      "21926.575127380434\n",
      "correct:  43943\n",
      "wrong:  6057\n",
      "accuracy: 87.886 %\n",
      "error: 12.114 %\n",
      "21864.284250922818\n",
      "correct:  43956\n",
      "wrong:  6044\n",
      "accuracy: 87.912 %\n",
      "error: 12.088 %\n",
      "21802.97737905095\n",
      "correct:  43974\n",
      "wrong:  6026\n",
      "accuracy: 87.948 %\n",
      "error: 12.052 %\n",
      "21742.65543095161\n",
      "correct:  43996\n",
      "wrong:  6004\n",
      "accuracy: 87.992 %\n",
      "error: 12.008 %\n",
      "21683.320089638048\n",
      "correct:  44010\n",
      "wrong:  5990\n",
      "accuracy: 88.02 %\n",
      "error: 11.98 %\n",
      "21624.94346996811\n",
      "correct:  44028\n",
      "wrong:  5972\n",
      "accuracy: 88.056 %\n",
      "error: 11.944 %\n",
      "21567.404343232185\n",
      "correct:  44042\n",
      "wrong:  5958\n",
      "accuracy: 88.084 %\n",
      "error: 11.916 %\n",
      "21510.789071903728\n",
      "100 iterations took 3826.793926715851 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(([None, array([[ 0.60613928, -0.33701888,  0.63026544, ..., -0.8136876 ,\n",
       "           -0.84591835, -0.76506445],\n",
       "          [-0.05243944,  0.24737549,  0.87427384, ..., -0.66517351,\n",
       "           -0.31128159,  0.71848396],\n",
       "          [-0.54279865, -0.68399193,  0.39070716, ...,  0.54099591,\n",
       "            0.79169095,  0.85685028],\n",
       "          ...,\n",
       "          [ 0.75308947, -0.91037367, -0.93297163, ..., -0.60764286,\n",
       "           -0.15162646, -0.70255753],\n",
       "          [ 0.34156202,  0.14444575, -0.45227531, ...,  0.89112712,\n",
       "            0.6743652 ,  0.91377555],\n",
       "          [ 0.25573907, -0.80089293, -0.52038244, ..., -0.81595794,\n",
       "            0.27615302, -0.0962647 ]])],\n",
       "  [None,\n",
       "   array([-0.91679258, -0.43252715,  0.45090683, -0.40294546,  0.1995021 ,\n",
       "           0.82141893,  0.41420664, -0.28171704, -1.24371928, -0.5385102 ])]),\n",
       " ([176973.66519770512,\n",
       "   122368.66139624966,\n",
       "   96210.09302097904,\n",
       "   80893.85712997772,\n",
       "   70947.89240534272,\n",
       "   63995.13015699213,\n",
       "   58849.79254094217,\n",
       "   54871.116992258336,\n",
       "   51688.964925836815,\n",
       "   49075.84210750774,\n",
       "   46885.24992997189,\n",
       "   45017.900109317095,\n",
       "   43403.71548763703,\n",
       "   41992.25667716676,\n",
       "   40745.6364934652,\n",
       "   39634.84250325803,\n",
       "   38637.60365034283,\n",
       "   37735.99612993693,\n",
       "   36915.966386993634,\n",
       "   36165.996039681086,\n",
       "   35476.84620026517,\n",
       "   34840.81940978856,\n",
       "   34251.50665351602,\n",
       "   33703.358272848025,\n",
       "   33191.91464640616,\n",
       "   32713.3344339855,\n",
       "   32264.13421399353,\n",
       "   31841.390910325445,\n",
       "   31442.675409121537,\n",
       "   31065.799479660258,\n",
       "   30708.794755899544,\n",
       "   30370.014351830116,\n",
       "   30047.879399470763,\n",
       "   29741.16601296312,\n",
       "   29448.510827724316,\n",
       "   29168.957452565162,\n",
       "   28901.53231212655,\n",
       "   28645.43435992689,\n",
       "   28399.791025384042,\n",
       "   28163.89745770969,\n",
       "   27937.134634817554,\n",
       "   27718.967048105205,\n",
       "   27508.82072227809,\n",
       "   27306.204218792307,\n",
       "   27110.696162020464,\n",
       "   26921.862376081328,\n",
       "   26739.336479811132,\n",
       "   26562.753670886636,\n",
       "   26391.79963379803,\n",
       "   26226.15606024954,\n",
       "   26065.612526691173,\n",
       "   25909.870429559385,\n",
       "   25758.72635705395,\n",
       "   25611.89729664844,\n",
       "   25469.206155895834,\n",
       "   25330.50884709619,\n",
       "   25195.54236557084,\n",
       "   25064.14803531062,\n",
       "   24936.176850095933,\n",
       "   24811.57465217781,\n",
       "   24690.088727051287,\n",
       "   24571.619315322478,\n",
       "   24456.069797262215,\n",
       "   24343.25723350797,\n",
       "   24233.126771258892,\n",
       "   24125.566114672252,\n",
       "   24020.45286407099,\n",
       "   23917.747385637525,\n",
       "   23817.33237598851,\n",
       "   23719.115321216348,\n",
       "   23623.016581279902,\n",
       "   23528.993389419382,\n",
       "   23436.96657415548,\n",
       "   23346.853371884295,\n",
       "   23258.549075415547,\n",
       "   23172.05571860284,\n",
       "   23087.26283222366,\n",
       "   23004.152985058194,\n",
       "   22922.664905625843,\n",
       "   22842.73832108946,\n",
       "   22764.315166030858,\n",
       "   22687.367230641423,\n",
       "   22611.831968222035,\n",
       "   22537.69950283335,\n",
       "   22464.914149005086,\n",
       "   22393.41634470718,\n",
       "   22323.221005755513,\n",
       "   22254.235406954875,\n",
       "   22186.44810979532,\n",
       "   22119.844267408054,\n",
       "   22054.34985241541,\n",
       "   21989.932257584445,\n",
       "   21926.575127380434,\n",
       "   21864.284250922818,\n",
       "   21802.97737905095,\n",
       "   21742.65543095161,\n",
       "   21683.320089638048,\n",
       "   21624.94346996811,\n",
       "   21567.404343232185,\n",
       "   21510.789071903728],\n",
       "  [0.27044,\n",
       "   0.43424,\n",
       "   0.52444,\n",
       "   0.58454,\n",
       "   0.62688,\n",
       "   0.65934,\n",
       "   0.68424,\n",
       "   0.70418,\n",
       "   0.72096,\n",
       "   0.73402,\n",
       "   0.74496,\n",
       "   0.7549,\n",
       "   0.76404,\n",
       "   0.7711,\n",
       "   0.77748,\n",
       "   0.78364,\n",
       "   0.78904,\n",
       "   0.79366,\n",
       "   0.79854,\n",
       "   0.80268,\n",
       "   0.8062,\n",
       "   0.80958,\n",
       "   0.81248,\n",
       "   0.81518,\n",
       "   0.81834,\n",
       "   0.82058,\n",
       "   0.82302,\n",
       "   0.82474,\n",
       "   0.82702,\n",
       "   0.82874,\n",
       "   0.83104,\n",
       "   0.83296,\n",
       "   0.83456,\n",
       "   0.83638,\n",
       "   0.8378,\n",
       "   0.83918,\n",
       "   0.84056,\n",
       "   0.84182,\n",
       "   0.84288,\n",
       "   0.8444,\n",
       "   0.84576,\n",
       "   0.84686,\n",
       "   0.84778,\n",
       "   0.84878,\n",
       "   0.84992,\n",
       "   0.85102,\n",
       "   0.852,\n",
       "   0.8528,\n",
       "   0.854,\n",
       "   0.855,\n",
       "   0.85596,\n",
       "   0.8568,\n",
       "   0.85762,\n",
       "   0.85846,\n",
       "   0.85912,\n",
       "   0.8597,\n",
       "   0.86054,\n",
       "   0.86132,\n",
       "   0.86216,\n",
       "   0.86284,\n",
       "   0.86356,\n",
       "   0.86424,\n",
       "   0.86488,\n",
       "   0.86532,\n",
       "   0.8659,\n",
       "   0.8665,\n",
       "   0.86706,\n",
       "   0.86782,\n",
       "   0.86856,\n",
       "   0.86918,\n",
       "   0.8696,\n",
       "   0.87026,\n",
       "   0.87072,\n",
       "   0.87096,\n",
       "   0.87136,\n",
       "   0.87176,\n",
       "   0.87206,\n",
       "   0.87242,\n",
       "   0.87278,\n",
       "   0.8732,\n",
       "   0.87358,\n",
       "   0.87406,\n",
       "   0.87456,\n",
       "   0.87498,\n",
       "   0.87554,\n",
       "   0.87576,\n",
       "   0.87626,\n",
       "   0.8767,\n",
       "   0.877,\n",
       "   0.87744,\n",
       "   0.87772,\n",
       "   0.87804,\n",
       "   0.87856,\n",
       "   0.87886,\n",
       "   0.87912,\n",
       "   0.87948,\n",
       "   0.87992,\n",
       "   0.8802,\n",
       "   0.88056,\n",
       "   0.88084]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = Neural_Network(identity, identityPrime, softMaxCross, softMaxCrossPrime, shape = [28**2, 10])\n",
    "f.gradientDescent(train_set, 100, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  44053\n",
      "wrong:  5947\n",
      "accuracy: 88.106 %\n",
      "error: 11.894 %\n",
      "21455.04636361138\n",
      "correct:  44065\n",
      "wrong:  5935\n",
      "accuracy: 88.13 %\n",
      "error: 11.87 %\n",
      "21400.14989962742\n",
      "correct:  44079\n",
      "wrong:  5921\n",
      "accuracy: 88.158 %\n",
      "error: 11.842 %\n",
      "21346.060182563273\n",
      "correct:  44093\n",
      "wrong:  5907\n",
      "accuracy: 88.186 %\n",
      "error: 11.814 %\n",
      "21292.80126782138\n",
      "correct:  44106\n",
      "wrong:  5894\n",
      "accuracy: 88.212 %\n",
      "error: 11.788 %\n",
      "21240.294088349314\n",
      "correct:  44118\n",
      "wrong:  5882\n",
      "accuracy: 88.236 %\n",
      "error: 11.764 %\n",
      "21188.558601841527\n",
      "correct:  44132\n",
      "wrong:  5868\n",
      "accuracy: 88.264 %\n",
      "error: 11.736 %\n",
      "21137.563144981355\n",
      "correct:  44145\n",
      "wrong:  5855\n",
      "accuracy: 88.29 %\n",
      "error: 11.71 %\n",
      "21087.284263118592\n",
      "correct:  44162\n",
      "wrong:  5838\n",
      "accuracy: 88.324 %\n",
      "error: 11.676 %\n",
      "21037.738593488626\n",
      "correct:  44172\n",
      "wrong:  5828\n",
      "accuracy: 88.344 %\n",
      "error: 11.656 %\n",
      "20988.8981638964\n",
      "10 iterations took 382.0187785625458 seconds.\n"
     ]
    }
   ],
   "source": [
    "train1 = f.gradientDescent(train_set, 10, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  44179\n",
      "wrong:  5821\n",
      "accuracy: 88.358 %\n",
      "error: 11.642 %\n",
      "20940.733036917758\n",
      "correct:  44189\n",
      "wrong:  5811\n",
      "accuracy: 88.378 %\n",
      "error: 11.622 %\n",
      "20893.240088041013\n",
      "correct:  44203\n",
      "wrong:  5797\n",
      "accuracy: 88.406 %\n",
      "error: 11.594 %\n",
      "20846.385263557222\n",
      "correct:  44202\n",
      "wrong:  5798\n",
      "accuracy: 88.404 %\n",
      "error: 11.596 %\n",
      "20800.17251774737\n",
      "correct:  44211\n",
      "wrong:  5789\n",
      "accuracy: 88.422 %\n",
      "error: 11.578 %\n",
      "20754.596101059728\n",
      "correct:  44217\n",
      "wrong:  5783\n",
      "accuracy: 88.434 %\n",
      "error: 11.566 %\n",
      "20709.608574287067\n",
      "correct:  44231\n",
      "wrong:  5769\n",
      "accuracy: 88.462 %\n",
      "error: 11.538 %\n",
      "20665.204336880714\n",
      "correct:  44242\n",
      "wrong:  5758\n",
      "accuracy: 88.484 %\n",
      "error: 11.516 %\n",
      "20621.365847753837\n",
      "correct:  44253\n",
      "wrong:  5747\n",
      "accuracy: 88.506 %\n",
      "error: 11.494 %\n",
      "20578.132258768863\n",
      "correct:  44274\n",
      "wrong:  5726\n",
      "accuracy: 88.548 %\n",
      "error: 11.452 %\n",
      "20535.427764257434\n",
      "correct:  44286\n",
      "wrong:  5714\n",
      "accuracy: 88.572 %\n",
      "error: 11.428 %\n",
      "20493.270902800443\n",
      "correct:  44299\n",
      "wrong:  5701\n",
      "accuracy: 88.598 %\n",
      "error: 11.402 %\n",
      "20451.66958010594\n",
      "correct:  44310\n",
      "wrong:  5690\n",
      "accuracy: 88.62 %\n",
      "error: 11.38 %\n",
      "20410.586156781377\n",
      "correct:  44329\n",
      "wrong:  5671\n",
      "accuracy: 88.658 %\n",
      "error: 11.342 %\n",
      "20370.014746632598\n",
      "correct:  44335\n",
      "wrong:  5665\n",
      "accuracy: 88.67 %\n",
      "error: 11.33 %\n",
      "20329.96664128954\n",
      "correct:  44347\n",
      "wrong:  5653\n",
      "accuracy: 88.694 %\n",
      "error: 11.306 %\n",
      "20290.397626605893\n",
      "correct:  44358\n",
      "wrong:  5642\n",
      "accuracy: 88.716 %\n",
      "error: 11.284 %\n",
      "20251.280818436026\n",
      "correct:  44369\n",
      "wrong:  5631\n",
      "accuracy: 88.738 %\n",
      "error: 11.262 %\n",
      "20212.629039906198\n",
      "correct:  44382\n",
      "wrong:  5618\n",
      "accuracy: 88.764 %\n",
      "error: 11.236 %\n",
      "20174.4665567039\n",
      "correct:  44391\n",
      "wrong:  5609\n",
      "accuracy: 88.782 %\n",
      "error: 11.218 %\n",
      "20136.75395954506\n",
      "20 iterations took 776.6995122432709 seconds.\n"
     ]
    }
   ],
   "source": [
    "train0 = f.gradientDescent(train_set, 20, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingData[\"training0\"] = train0\n",
    "trainingData[\"training1\"] = train1\n",
    "with open('trainingData.pickle', 'wb') as handle:\n",
    "    pickle.dump(trainingData, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('trainingData.pickle', 'rb') as handle:\n",
    "    lastTrainData = pickle.load(handle)\n",
    "w, b = lastTrainData[\"training0\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  8958\n",
      "wrong:  1042\n",
      "accuracy: 89.58 %\n",
      "error: 10.42 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8958, 10000)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Neural_Network(identity, identityPrime, softMaxCross, softMaxCrossPrime, shape = [28**2, 10])\n",
    "g.weights = w\n",
    "g.bias = b\n",
    "g.validation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 iterations took 72.29792308807373 seconds.\n"
     ]
    }
   ],
   "source": [
    "train2 = g.gradientDescent(train_set, 10, 0.5, suppressPrint = True, costInterval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  8971\n",
      "wrong:  1029\n",
      "accuracy: 89.71 %\n",
      "error: 10.29 %\n",
      "correct:  8959\n",
      "wrong:  1041\n",
      "accuracy: 89.59 %\n",
      "error: 10.41 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8959, 10000)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.validation(valid_set)\n",
    "g.validation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 iterations took 200.07256722450256 seconds.\n"
     ]
    }
   ],
   "source": [
    "train3 = g.gradientDescent(train_set, 30, 0.5, suppressPrint = True, costInterval = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  8988\n",
      "wrong:  1012\n",
      "accuracy: 89.88 %\n",
      "error: 10.12 %\n",
      "correct:  8951\n",
      "wrong:  1049\n",
      "accuracy: 89.51 %\n",
      "error: 10.49 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8951, 10000)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.validation(valid_set)\n",
    "g.validation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 iterations took 206.80948734283447 seconds.\n"
     ]
    }
   ],
   "source": [
    "train4 = g.gradientDescent(train_set, 30, 0.5, suppressPrint = True, costInterval = 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  44575\n",
      "wrong:  5425\n",
      "accuracy: 89.15 %\n",
      "error: 10.85 %\n",
      "19469.40282616296\n",
      "correct:  44589\n",
      "wrong:  5411\n",
      "accuracy: 89.178 %\n",
      "error: 10.822 %\n",
      "19406.162760274554\n",
      "correct:  44613\n",
      "wrong:  5387\n",
      "accuracy: 89.226 %\n",
      "error: 10.774 %\n",
      "19344.992752706858\n",
      "correct:  44631\n",
      "wrong:  5369\n",
      "accuracy: 89.262 %\n",
      "error: 10.738 %\n",
      "19285.646264319927\n",
      "40 iterations took 275.68500304222107 seconds.\n"
     ]
    }
   ],
   "source": [
    "train5 = g.gradientDescent(train_set, 40, 0.5, costInterval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  9004\n",
      "wrong:  996\n",
      "accuracy: 90.04 %\n",
      "error: 9.96 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9004, 10000)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.validation(valid_set)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
